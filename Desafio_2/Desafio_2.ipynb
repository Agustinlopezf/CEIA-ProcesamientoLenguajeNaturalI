{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8589956a",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
    "\n",
    "\n",
    "# Procesamiento de lenguaje natural\n",
    "## Custom embedddings con Gensim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04d1155",
   "metadata": {},
   "source": [
    "### Objetivo\n",
    "El objetivo es utilizar documentos / corpus para crear embeddings de palabras basado en ese contexto. Se utilizará canciones de bandas para generar los embeddings, es decir, que los vectores tendrán la forma en función de como esa banda haya utilizado las palabras en sus canciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "746cab1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977ae22e",
   "metadata": {},
   "source": [
    "### Desafío 2\n",
    "- Crear sus propios vectores con Gensim basado en lo visto en clase con otro dataset.\n",
    "- Probar términos de interés y explicar similitudes en el espacio de embeddings (sacar conclusiones entre palabras similitudes y diferencias).\n",
    "- Graficarlos.\n",
    "- Obtener conclusiones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037cdf93",
   "metadata": {},
   "source": [
    "### Datos\n",
    "Como fuente de datos de utilizarán las novelas de \"A Song of Ice and Fire\" de George Martin. Las mismas fueron obtenidas de Kaggle:\n",
    "https://www.kaggle.com/datasets/saurabhbadole/game-of-thrones-book-dataset/data\n",
    "\n",
    "El dataset cotiene cinco archivos de texto, cada uno con un libro de la saga. Se comienza concatenando todos los archivos en un solo corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1604869c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "lista_libros = [\n",
    "    \"1 - A Game of Thrones.txt\",\n",
    "    \"2 - A Clash of Kings.txt\",\n",
    "    \"3 - A Storm of Swords.txt\",\n",
    "    \"4 - A Feast for Crows.txt\",\n",
    "    \"5 - A Dance with Dragons.txt\"\n",
    "]\n",
    "\n",
    "# Crear un corpus en memoria\n",
    "corpus = \"\"\n",
    "\n",
    "for nombre in lista_libros:\n",
    "    ruta_libro = os.path.join(\"./Dataset\", nombre)\n",
    "    with open(ruta_libro, \"r\", encoding=\"latin-1\") as f:\n",
    "        contenido = f.read()\n",
    "        corpus += contenido + \"\\n\"  # agrega un salto de línea entre libros\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6209a9b6",
   "metadata": {},
   "source": [
    "Se muestran las primeras líneas del texto completo, la cantidad de líneas y caracteres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad15dc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeras 10 líneas:\n",
      "A Game Of Thrones \n",
      "Book One of A Song of Ice and Fire \n",
      "By George R. R. Martin \n",
      "PROLOGUE \n",
      "\"We should start back,\" Gared urged as the woods began to grow dark around them. \"The wildlings are \n",
      "dead.\" \n",
      "\"Do the dead frighten you?\" Ser Waymar Royce asked with just the hint of a smile. \n",
      "Gared did not rise to the bait. He was an old man, past fifty, and he had seen the lordlings come and go. \n",
      "\"Dead is dead,\" he said. \"We have no business with the dead.\" \n",
      "\"Are they dead?\" Royce asked softly. \"What proof have we?\" \n",
      "\n",
      "Cantidad de líneas: 125666\n",
      "Cantidad total de caracteres: 9778338\n"
     ]
    }
   ],
   "source": [
    "# Dividir el corpus en líneas\n",
    "lineas = corpus.splitlines()  # crea una lista de líneas\n",
    "\n",
    "# Mostrar las primeras 10 líneas\n",
    "print(\"Primeras 10 líneas:\")\n",
    "print(\"\\n\".join(lineas[:10]))\n",
    "\n",
    "# Cantidad de líneas\n",
    "print(\"\\nCantidad de líneas:\", len(lineas))\n",
    "\n",
    "# Cantidad total de caracteres\n",
    "print(\"Cantidad total de caracteres:\", len(corpus))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9edf13",
   "metadata": {},
   "source": [
    "# Preprocesamiento de texto para embeddings\n",
    "\n",
    "Antes de entrenar embeddings, se aplican varios pasos de limpieza y tokenización del texto:\n",
    "\n",
    "- **Convertir a minúsculas:**  \n",
    "  Se unifican todas las palabras para evitar duplicados debidos a mayúsculas/minúsculas.\n",
    "\n",
    "- **Eliminar encabezados y secciones no deseadas:**  \n",
    "  Se eliminan nombres de libros, capítulos, autor, prólogo, dedicatoria, tabla de contenidos, notas de cronología y numeración de páginas.\n",
    "\n",
    "- **Separar en oraciones:**  \n",
    "  Se divide el texto usando signos de puntuación (`.`, `?`, `!`) para preservar la estructura básica de las oraciones, incluyendo diálogos y frases cortas.\n",
    "\n",
    "- **Limpiar cada oración:**  \n",
    "  Se eliminan números y puntuación interna, y se normalizan espacios en blanco para obtener texto limpio.\n",
    "\n",
    "- **Tokenización y eliminación de *stopwords*:**  \n",
    "  Cada oración se convierte en una lista de palabras y se eliminan las palabras vacías (como \"the\", \"and\", \"of\") que no aportan significado semántico.\n",
    "\n",
    "\n",
    "**Nota:** Algunas oraciones resultan muy cortas después de este proceso, especialmente diálogos o frases muy breves. Como alternativa, se evalúa realizar la tokenización por párrafos completos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "339feaa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de oraciones tokenizadas: 155054\n",
      "Ejemplo de primera oración tokenizada: ['start', 'back', 'gared', 'urged', 'woods', 'began', 'grow', 'dark', 'around']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def preprocesar_texto_por_oracion(text):\n",
    "    \"\"\"\n",
    "    Recibe un texto completo y devuelve una lista de oraciones,\n",
    "    cada una representada como lista de tokens limpios.\n",
    "    Esta versión no depende de NLTK punkt.\n",
    "    \"\"\"\n",
    "    # 1. Pasar a minúsculas\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Eliminar encabezados no deseados\n",
    "    patterns_to_remove = [\n",
    "        r\"a game of thrones\",\n",
    "        r\"a clash of kings\",\n",
    "        r\"a storm of swords\",\n",
    "        r\"a feast for crows\",\n",
    "        r\"a dance with dragons\",\n",
    "        r\"book [^\\n]+\",          # líneas tipo \"Book One of...\"\n",
    "        r\"by george r\\. r\\. martin\",\n",
    "        r\"prologue\",\n",
    "        r\"dedication\",\n",
    "        r\"contents\",\n",
    "        r\"a note on chronology\",\n",
    "        r\"a cavil on chronology\",\n",
    "        r\"version history.*\",\n",
    "        r\"page \\d+\",             # \"Page XX\"\n",
    "    ]\n",
    "    for pat in patterns_to_remove:\n",
    "        text = re.sub(pat, \" \", text)\n",
    "    \n",
    "    # 3. Separar en “oraciones” usando signos de puntuación\n",
    "    oraciones = re.split(r'[.!?]+', text)\n",
    "    oraciones = [s.strip() for s in oraciones if s.strip()]\n",
    "    \n",
    "    # 4. Tokenizar palabras y limpiar cada oración\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    corpus_tokens = []\n",
    "    for oracion in oraciones:\n",
    "        # Eliminar números y puntuación dentro de la oración\n",
    "        clean_oracion = re.sub(r'\\d+', ' ', oracion)\n",
    "        clean_oracion = clean_oracion.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "        clean_oracion = re.sub(r'\\s+', ' ', clean_oracion).strip()\n",
    "        \n",
    "        # Tokenizar por espacios y eliminar stopwords\n",
    "        tokens = [w for w in clean_oracion.split() if w not in stop_words]\n",
    "        if tokens:\n",
    "            corpus_tokens.append(tokens)\n",
    "    \n",
    "    return corpus_tokens\n",
    "\n",
    "# Aplicar al corpus completo\n",
    "corpus_tokens = preprocesar_texto_por_oracion(corpus)\n",
    "\n",
    "print(\"Cantidad de oraciones tokenizadas:\", len(corpus_tokens))\n",
    "print(\"Ejemplo de primera oración tokenizada:\", corpus_tokens[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5937da",
   "metadata": {},
   "source": [
    "Visualizamos los tokens de las 20 primeras oraciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eca8ee02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['start', 'back', 'gared', 'urged', 'woods', 'began', 'grow', 'dark', 'around']\n",
      "['wildlings', 'dead']\n",
      "['dead', 'frighten']\n",
      "['ser', 'waymar', 'royce', 'asked', 'hint', 'smile']\n",
      "['gared', 'rise', 'bait']\n",
      "['old', 'man', 'past', 'fifty', 'seen', 'lordlings', 'come', 'go']\n",
      "['dead', 'dead', 'said']\n",
      "['business', 'dead']\n",
      "['dead']\n",
      "['royce', 'asked', 'softly']\n",
      "['proof']\n",
      "['saw', 'gared', 'said']\n",
      "['says', 'dead', 'thats', 'proof', 'enough']\n",
      "['known', 'would', 'drag', 'quarrel', 'sooner', 'later']\n",
      "['wished', 'later', 'rather', 'sooner']\n",
      "['mother', 'told', 'dead', 'men', 'sing', 'songs', 'put']\n",
      "['wet', 'nurse', 'said', 'thing', 'royce', 'replied']\n",
      "['never', 'believe', 'anything', 'hear', 'womans', 'tit']\n",
      "['things', 'learned', 'even', 'dead']\n",
      "['voice', 'echoed', 'loud', 'twilit', 'forest']\n"
     ]
    }
   ],
   "source": [
    "for token in corpus_tokens[:20]:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69959d5d",
   "metadata": {},
   "source": [
    "#### Alternativa: Tokenizar por párrafos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55aca1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de párrafos tokenizados: 118164\n",
      "Ejemplo de primer párrafo tokenizado: ['start', 'back', 'gared', 'urged', 'woods', 'began', 'grow', 'dark', 'around', 'wildlings']\n",
      "['start', 'back', 'gared', 'urged', 'woods', 'began', 'grow', 'dark', 'around', 'wildlings']\n",
      "['dead']\n",
      "['dead', 'frighten', 'ser', 'waymar', 'royce', 'asked', 'hint', 'smile']\n",
      "['gared', 'rise', 'bait', 'old', 'man', 'past', 'fifty', 'seen', 'lordlings', 'come', 'go']\n",
      "['dead', 'dead', 'said', 'business', 'dead']\n",
      "['dead', 'royce', 'asked', 'softly', 'proof']\n",
      "['saw', 'gared', 'said', 'says', 'dead', 'thats', 'proof', 'enough']\n",
      "['known', 'would', 'drag', 'quarrel', 'sooner', 'later', 'wished', 'later', 'rather']\n",
      "['sooner', 'mother', 'told', 'dead', 'men', 'sing', 'songs', 'put']\n",
      "['wet', 'nurse', 'said', 'thing', 'royce', 'replied', 'never', 'believe', 'anything', 'hear', 'womans']\n",
      "['tit', 'things', 'learned', 'even', 'dead', 'voice', 'echoed', 'loud', 'twilit', 'forest']\n",
      "['long', 'ride', 'us', 'gared', 'pointed', 'eight', 'days', 'maybe', 'nine', 'night', 'falling']\n",
      "['ser', 'waymar', 'royce', 'glanced', 'sky', 'disinterest', 'every', 'day', 'time']\n",
      "['unmanned', 'dark', 'gared']\n",
      "['could', 'see', 'tightness', 'around', 'gareds', 'mouth', 'barely', 'sup']\n",
      "['pressed', 'anger', 'eyes', 'thick', 'black', 'hood', 'cloak', 'gared', 'spent', 'forty', 'years']\n",
      "['nights', 'watch', 'man', 'boy', 'accustomed', 'made', 'light', 'yet']\n",
      "['wounded', 'pride', 'could', 'sense', 'something', 'else', 'older', 'man', 'could', 'taste']\n",
      "['nervous', 'tension', 'came', 'perilous', 'close', 'fear']\n",
      "['shared', 'unease', 'four', 'years', 'wall', 'first', 'time', 'sent', 'beyond']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def preprocesar_texto_por_parrafo(text):\n",
    "    \"\"\"\n",
    "    Recibe un texto completo y devuelve una lista de párrafos,\n",
    "    cada uno representado como lista de tokens limpios.\n",
    "    \"\"\"\n",
    "    # 1. Pasar a minúsculas\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2. Reemplazar caracteres especiales comunes\n",
    "    text = text.replace(\"\\x97\", \" \")  # guion largo\n",
    "    text = text.replace(\"\\x96\", \" \")  # guion corto\n",
    "    text = text.replace(\"\\x91\", \"'\").replace(\"\\x92\", \"'\")  # comillas simples\n",
    "    text = text.replace(\"\\x93\", '\"').replace(\"\\x94\", '\"')  # comillas dobles\n",
    "    # eliminar cualquier otro caracter no ASCII\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    \n",
    "    # 3. Eliminar encabezados no deseados\n",
    "    patterns_to_remove = [\n",
    "        r\"a game of thrones\",\n",
    "        r\"a clash of kings\",\n",
    "        r\"a storm of swords\",\n",
    "        r\"a feast for crows\",\n",
    "        r\"a dance with dragons\",\n",
    "        r\"book [^\\n]+\",\n",
    "        r\"by george r\\. r\\. martin\",\n",
    "        r\"prologue\",\n",
    "        r\"dedication\",\n",
    "        r\"contents\",\n",
    "        r\"a note on chronology\",\n",
    "        r\"a cavil on chronology\",\n",
    "        r\"version history.*\",\n",
    "        r\"page \\d+\",\n",
    "    ]\n",
    "    for pat in patterns_to_remove:\n",
    "        text = re.sub(pat, \" \", text)\n",
    "    \n",
    "    # 4. Separar en párrafos usando saltos de línea\n",
    "    parrafos = text.split(\"\\n\")\n",
    "    parrafos = [p.strip() for p in parrafos if p.strip()]\n",
    "    \n",
    "    # 5. Limpiar cada párrafo y tokenizar\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    corpus_tokens = []\n",
    "    for parrafo in parrafos:\n",
    "        # Eliminar números y puntuación\n",
    "        clean_parrafo = re.sub(r'\\d+', ' ', parrafo)\n",
    "        clean_parrafo = clean_parrafo.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "        clean_parrafo = re.sub(r'\\s+', ' ', clean_parrafo).strip()\n",
    "        \n",
    "        # Tokenizar por espacios y eliminar stopwords\n",
    "        tokens = [w for w in clean_parrafo.split() if w not in stop_words]\n",
    "        if tokens:\n",
    "            corpus_tokens.append(tokens)\n",
    "    \n",
    "    return corpus_tokens\n",
    "\n",
    "# Aplicar al corpus completo\n",
    "corpus_tokens_parrafo = preprocesar_texto_por_parrafo(corpus)\n",
    "\n",
    "print(\"Cantidad de párrafos tokenizados:\", len(corpus_tokens_parrafo))\n",
    "print(\"Ejemplo de primer párrafo tokenizado:\", corpus_tokens_parrafo[0])\n",
    "\n",
    "for token in corpus_tokens_parrafo[:20]:\n",
    "    print(token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1af4889",
   "metadata": {},
   "source": [
    "### Entrenamiento de modelos de embeddings:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcc459b",
   "metadata": {},
   "source": [
    "Agregamos la función de callback vista en clase para informar el loss de cada época:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ff737b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "# Durante el entrenamiento gensim por defecto no informa el \"loss\" en cada época\n",
    "# Sobrecargamos el callback para poder tener esta información\n",
    "class callback(CallbackAny2Vec):\n",
    "    \"\"\"\n",
    "    Callback to print loss after each epoch\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        if self.epoch == 0:\n",
    "            print('Loss after epoch {}: {}'.format(self.epoch, loss))\n",
    "        else:\n",
    "            print('Loss after epoch {}: {}'.format(self.epoch, loss- self.loss_previous_step))\n",
    "        self.epoch += 1\n",
    "        self.loss_previous_step = loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c80c4d0",
   "metadata": {},
   "source": [
    "Entrenamos dos variantes del modelo, tanto con **CBOW** como con **Skipgram**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b79a32",
   "metadata": {},
   "source": [
    "Si bien en el ejemplo en clase entrenamos embeddings con dimensión 300, dado que observamos que las oraciones son bastante cortas, disminuimos la dimensionalidad del embedding a 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6a8714",
   "metadata": {},
   "source": [
    "#### Entrenamiento modelo Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e680951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crearmos el modelo generador de vectores\n",
    "# En este caso utilizaremos la estructura modelo Skipgram\n",
    "w2v_model_sg = Word2Vec(min_count=5,    # frecuencia mínima de palabra para incluirla en el vocabulario\n",
    "                     window=2,       # cant de palabras antes y desp de la predicha\n",
    "                     vector_size=100,       # dimensionalidad de los vectores \n",
    "                     negative=20,    # cantidad de negative samples... 0 es no se usa\n",
    "                     workers=5,      # si tienen más cores pueden cambiar este valor\n",
    "                     sg=1)           # modelo 0:CBOW  1:skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcf83470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de docs en el corpus: 155054\n",
      "Cantidad de words distintas en el corpus: 12698\n"
     ]
    }
   ],
   "source": [
    "w2v_model_sg.build_vocab(corpus_tokens)\n",
    "\n",
    "# Cantidad de filas/docs encontradas en el corpus\n",
    "print(\"Cantidad de docs en el corpus:\", w2v_model_sg.corpus_count)\n",
    "\n",
    "# Cantidad de palabras encontradas en el corpus\n",
    "print(\"Cantidad de words distintas en el corpus:\", len(w2v_model_sg.wv.index_to_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e536828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 1710212.5\n",
      "Loss after epoch 1: 1300642.25\n",
      "Loss after epoch 2: 1220713.75\n",
      "Loss after epoch 3: 1099945.5\n",
      "Loss after epoch 4: 1030939.5\n",
      "Loss after epoch 5: 1068403.5\n",
      "Loss after epoch 6: 1006568.0\n",
      "Loss after epoch 7: 941937.0\n",
      "Loss after epoch 8: 930343.0\n",
      "Loss after epoch 9: 972933.0\n",
      "Loss after epoch 10: 916532.0\n",
      "Loss after epoch 11: 901545.0\n",
      "Loss after epoch 12: 943384.0\n",
      "Loss after epoch 13: 891947.0\n",
      "Loss after epoch 14: 849576.0\n",
      "Loss after epoch 15: 973904.0\n",
      "Loss after epoch 16: 784144.0\n",
      "Loss after epoch 17: 855848.0\n",
      "Loss after epoch 18: 822212.0\n",
      "Loss after epoch 19: 761792.0\n",
      "Loss after epoch 20: 807646.0\n",
      "Loss after epoch 21: 850472.0\n",
      "Loss after epoch 22: 804488.0\n",
      "Loss after epoch 23: 848590.0\n",
      "Loss after epoch 24: 794140.0\n",
      "Loss after epoch 25: 830432.0\n",
      "Loss after epoch 26: 843522.0\n",
      "Loss after epoch 27: 826294.0\n",
      "Loss after epoch 28: 833718.0\n",
      "Loss after epoch 29: 793918.0\n",
      "Loss after epoch 30: 784144.0\n",
      "Loss after epoch 31: 796608.0\n",
      "Loss after epoch 32: 728536.0\n",
      "Loss after epoch 33: 778384.0\n",
      "Loss after epoch 34: 819100.0\n",
      "Loss after epoch 35: 775150.0\n",
      "Loss after epoch 36: 794608.0\n",
      "Loss after epoch 37: 759576.0\n",
      "Loss after epoch 38: 719868.0\n",
      "Loss after epoch 39: 754152.0\n",
      "Loss after epoch 40: 713676.0\n",
      "Loss after epoch 41: 706540.0\n",
      "Loss after epoch 42: 753224.0\n",
      "Loss after epoch 43: 754020.0\n",
      "Loss after epoch 44: 706040.0\n",
      "Loss after epoch 45: 708836.0\n",
      "Loss after epoch 46: 735088.0\n",
      "Loss after epoch 47: 715732.0\n",
      "Loss after epoch 48: 748300.0\n",
      "Loss after epoch 49: 751676.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(44091254, 47474250)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenamos el modelo generador de vectores\n",
    "# Utilizamos nuestro callback\n",
    "w2v_model_sg.train(corpus_tokens,\n",
    "                 total_examples=w2v_model_sg.corpus_count,\n",
    "                 epochs=50,\n",
    "                 compute_loss = True,\n",
    "                 callbacks=[callback()]\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e33b7b",
   "metadata": {},
   "source": [
    "#### Entrenamiento modelo CBOW:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33a41f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model_cbow = Word2Vec(min_count=5,    # frecuencia mínima de palabra para incluirla en el vocabulario\n",
    "                     window=2,       # cant de palabras antes y desp de la predicha\n",
    "                     vector_size=100,       # dimensionalidad de los vectores \n",
    "                     negative=20,    # cantidad de negative samples... 0 es no se usa\n",
    "                     workers=5,      # si tienen más cores pueden cambiar este valor\n",
    "                     sg=0)           # modelo 0:CBOW  1:skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "205a4d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 869420.9375\n",
      "Loss after epoch 1: 620627.8125\n",
      "Loss after epoch 2: 565155.25\n",
      "Loss after epoch 3: 490026.75\n",
      "Loss after epoch 4: 494456.5\n",
      "Loss after epoch 5: 488458.75\n",
      "Loss after epoch 6: 433861.5\n",
      "Loss after epoch 7: 440861.5\n",
      "Loss after epoch 8: 398077.0\n",
      "Loss after epoch 9: 406698.0\n",
      "Loss after epoch 10: 373864.0\n",
      "Loss after epoch 11: 404653.5\n",
      "Loss after epoch 12: 402789.5\n",
      "Loss after epoch 13: 389241.0\n",
      "Loss after epoch 14: 389332.5\n",
      "Loss after epoch 15: 373365.5\n",
      "Loss after epoch 16: 371654.5\n",
      "Loss after epoch 17: 376084.0\n",
      "Loss after epoch 18: 368897.5\n",
      "Loss after epoch 19: 369510.0\n",
      "Loss after epoch 20: 365713.0\n",
      "Loss after epoch 21: 372257.0\n",
      "Loss after epoch 22: 377429.0\n",
      "Loss after epoch 23: 383292.0\n",
      "Loss after epoch 24: 380198.0\n",
      "Loss after epoch 25: 372340.0\n",
      "Loss after epoch 26: 379201.0\n",
      "Loss after epoch 27: 371796.0\n",
      "Loss after epoch 28: 369310.0\n",
      "Loss after epoch 29: 364093.0\n",
      "Loss after epoch 30: 366635.0\n",
      "Loss after epoch 31: 355112.0\n",
      "Loss after epoch 32: 357708.0\n",
      "Loss after epoch 33: 351414.0\n",
      "Loss after epoch 34: 366503.0\n",
      "Loss after epoch 35: 353879.0\n",
      "Loss after epoch 36: 358907.0\n",
      "Loss after epoch 37: 358895.0\n",
      "Loss after epoch 38: 356175.0\n",
      "Loss after epoch 39: 359448.0\n",
      "Loss after epoch 40: 341407.0\n",
      "Loss after epoch 41: 289084.0\n",
      "Loss after epoch 42: 253564.0\n",
      "Loss after epoch 43: 264720.0\n",
      "Loss after epoch 44: 259478.0\n",
      "Loss after epoch 45: 244812.0\n",
      "Loss after epoch 46: 247844.0\n",
      "Loss after epoch 47: 261584.0\n",
      "Loss after epoch 48: 247292.0\n",
      "Loss after epoch 49: 265420.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(44090564, 47474250)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model_cbow.build_vocab(corpus_tokens)\n",
    "\n",
    "w2v_model_cbow.train(corpus_tokens,\n",
    "                 total_examples=w2v_model_cbow.corpus_count,\n",
    "                 epochs=50,\n",
    "                 compute_loss = True,\n",
    "                 callbacks=[callback()]\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024d1f9c",
   "metadata": {},
   "source": [
    "### Análisis de resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45880411",
   "metadata": {},
   "source": [
    "Evaluamos las 10 más palabras más similares, y las 10 menos similares, para distintos personajes, apellidos o palabras importantes de las novelas. En cada caso evaluamos tanto los modelos de CBOW como Skipgram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97554f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostrar_similares(model, palabra, topn=10, tipo_modelo= \"\"):\n",
    "    \"\"\"\n",
    "    Muestra una tabla con los n más similares (positivos)\n",
    "    y los n menos similares (negativos) a la palabra dada.\n",
    "    \"\"\"\n",
    "    positivos = model.wv.most_similar(positive=[palabra], topn=topn)\n",
    "    negativos = model.wv.most_similar(negative=[palabra], topn=topn)\n",
    "\n",
    "    # Convertir en DataFrames\n",
    "    df_pos = pd.DataFrame(positivos, columns=[\"Positivo\", \"Similitud\"])\n",
    "    df_neg = pd.DataFrame(negativos, columns=[\"Negativo\", \"Similitud\"])\n",
    "\n",
    "    # Combinar lado a lado\n",
    "    tabla = pd.concat([df_pos, df_neg], axis=1)\n",
    "\n",
    "    print(f\"\\nPalabra consultada: {palabra}\\n\")\n",
    "    if tipo_modelo:\n",
    "        print(f\"\\Modelo utilizado: {tipo_modelo}\\n\")\n",
    "    print(tabla.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8855e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Palabra consultada: tyrion\n",
      "\n",
      "\\Modelo utilizado: Skipgram\n",
      "\n",
      " Positivo  Similitud   Negativo  Similitud\n",
      "   cersei   0.739829    fishing   0.102873\n",
      "    jaime   0.713796 surrounded   0.058447\n",
      "    dwarf   0.674522    trained   0.049203\n",
      "    kevan   0.605468   southron   0.046657\n",
      "crookedly   0.596174      andal   0.041858\n",
      "  brienne   0.592341       the   0.035507\n",
      "   alayne   0.588232      badge   0.034745\n",
      "     dany   0.587753     mostly   0.030968\n",
      "    sansa   0.581602      swift   0.028459\n",
      "  catelyn   0.561408  attacking   0.022322\n",
      "\n",
      "Palabra consultada: tyrion\n",
      "\n",
      "\\Modelo utilizado: CBOW\n",
      "\n",
      "Positivo  Similitud    Negativo  Similitud\n",
      "   jaime   0.798096      called   0.412795\n",
      "  cersei   0.751257       knows   0.402898\n",
      "    dany   0.700668        plus   0.386118\n",
      " brienne   0.669359        the   0.384119\n",
      "   dwarf   0.668421      chiefs   0.381825\n",
      "   sansa   0.604185   consigned   0.376096\n",
      "   theon   0.592169     emerged   0.373043\n",
      "    shae   0.590458   smothered   0.368286\n",
      "    arya   0.588943    infested   0.363355\n",
      " catelyn   0.574756 accompanied   0.361774\n"
     ]
    }
   ],
   "source": [
    "mostrar_similares(w2v_model_sg, \"tyrion\", 10, \"Skipgram\")\n",
    "mostrar_similares(w2v_model_cbow, \"tyrion\", 10, \"CBOW\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e31f12",
   "metadata": {},
   "source": [
    "#### Similitud con palaba **Tyrion**\n",
    "- **Skipgram**:\n",
    "  - **Positivos**: Captura lazos familiares Lannister (\"cersei\" 0.74, \"jaime\" 0.69) y el rasgo distintivo \"dwarf\" (0.64). Incluye personajes narrativos clave como \"dany\", \"brienne\", \"sansa\" (~0.57-0.59) y términos de intriga como \"varys\" y \"alayne\".\n",
    "  - **Negativos**: Términos irrelevantes como \"fishing\", \"nests\" (<0.13).\n",
    "  - **Patrones**: Enfatiza relaciones específicas (familia, rasgos físicos) y tramas de intriga.\n",
    "- **CBOW**:\n",
    "  - **Positivos**: Prioriza relaciones familiares (\"jaime\" 0.78, \"cersei\" 0.75) y conexiones narrativas más amplias con Stark (\"catelyn\", \"ned\", \"arya\" ~0.59-0.61) y \"griff\" (0.55).\n",
    "  - **Negativos**: Términos genéricos como \"called\", \"knows\" (~0.36-0.42), menos específicos.\n",
    "  - **Patrones**: Captura contextos narrativos más generales, incluyendo conflictos Stark-Lannister."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1f2df26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Palabra consultada: stark\n",
      "\n",
      "\\Modelo utilizado: Skipgram\n",
      "\n",
      "Positivo  Similitud  Negativo  Similitud\n",
      "  starks   0.674441   barrels   0.077658\n",
      "  eddard   0.623175      jars   0.070255\n",
      " starks   0.606944  unwashed   0.062891\n",
      "   tully   0.586711  cookfire   0.058753\n",
      "    robb   0.572691   hopping   0.057505\n",
      "leobalds   0.554411     necks   0.052534\n",
      "eddards   0.547003  serpents   0.050519\n",
      "karstark   0.524763 mollander   0.050251\n",
      "direwolf   0.520917 onehanded   0.048464\n",
      " brandon   0.517103  bursting   0.047685\n",
      "\n",
      "Palabra consultada: stark\n",
      "\n",
      "\\Modelo utilizado: CBOW\n",
      "\n",
      "   Positivo  Similitud      Negativo  Similitud\n",
      "     starks   0.682775       grasses   0.435065\n",
      "    starks   0.616588          bees   0.408846\n",
      "      tully   0.487811       firepit   0.397636\n",
      "     father   0.469191 diamondshaped   0.394681\n",
      "grandfather   0.452066       incense   0.391934\n",
      "   karstark   0.448339           dew   0.390502\n",
      "    greyjoy   0.443791        slices   0.387929\n",
      " winterfell   0.442422         whale   0.387268\n",
      "       lysa   0.440769       buzzing   0.383401\n",
      "     robert   0.434807     mushrooms   0.376077\n"
     ]
    }
   ],
   "source": [
    "mostrar_similares(w2v_model_sg, \"stark\", 10, \"Skipgram\")\n",
    "mostrar_similares(w2v_model_cbow, \"stark\", 10, \"CBOW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08e6c2e",
   "metadata": {},
   "source": [
    "#### Similitud con palaba **Stark**\n",
    "- **Skipgram**:\n",
    "  - **Positivos**: Enfatiza la familia (\"eddard\" 0.65, \"robb\" 0.59) y el hogar (\"winterfell\" 0.53), con la alianza Tully (\"tully\" 0.54) y personajes secundarios (\"leobalds\", \"daryn\" ~0.51-0.52).\n",
    "  - **Negativos**: Términos irrelevantes como \"unwashed\", \"onions\" (<0.07).\n",
    "  - **Patrones**: Destaca la identidad nuclear de los Stark y su conexión geográfica.\n",
    "- **CBOW**:\n",
    "  - **Positivos**: Captura formas derivadas (\"starks\" 0.69, \"stark’s\" 0.63), alianzas (\"tully\" 0.50), y términos familiares (\"father\", \"grandfather\" ~0.45-0.47), con conexiones narrativas (\"greyjoy\", \"robert\" ~0.44).\n",
    "  - **Negativos**: Términos genéricos como \"grasses\", \"firepit\" (~0.39-0.45), menos específicos.\n",
    "  - **Patrones**: Enfatiza roles familiares y conexiones históricas más amplias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13e1d490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Palabra consultada: dragon\n",
      "\n",
      "\\Modelo utilizado: Skipgram\n",
      "\n",
      "   Positivo  Similitud Negativo  Similitud\n",
      "threeheaded   0.589148     luke   0.104582\n",
      "   dragons   0.585153  glances   0.093567\n",
      "    dragons   0.575616   orphan   0.091147\n",
      "   daenerys   0.553533     hugh   0.089918\n",
      "    dynasty   0.550802  collect   0.088536\n",
      "    viserys   0.546775     kyle   0.083490\n",
      "  stormborn   0.520053   feeble   0.081725\n",
      " targaryens   0.508092    cloud   0.078050\n",
      "  targaryen   0.503702    mudge   0.077691\n",
      "     vhagar   0.498106 probably   0.075495\n",
      "\n",
      "Palabra consultada: dragon\n",
      "\n",
      "\\Modelo utilizado: CBOW\n",
      "\n",
      " Positivo  Similitud   Negativo  Similitud\n",
      "  dragons   0.500124     cuffed   0.401113\n",
      "   drogon   0.460943   thrummed   0.397417\n",
      "conqueror   0.441674   supplied   0.384669\n",
      "    lions   0.432969      toyed   0.371589\n",
      "    bitch   0.408729   observed   0.352525\n",
      " dragons   0.400938     combed   0.348025\n",
      "     fire   0.397324  affection   0.347497\n",
      "  meraxes   0.387619    snoring   0.343151\n",
      "conquerer   0.386962 hungerford   0.340874\n",
      "     stag   0.385188       ser   0.336348\n"
     ]
    }
   ],
   "source": [
    "mostrar_similares(w2v_model_sg, \"dragon\", 10, \"Skipgram\")\n",
    "mostrar_similares(w2v_model_cbow, \"dragon\", 10, \"CBOW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254ebdd6",
   "metadata": {},
   "source": [
    "#### Similitud con palaba **Dragon**\n",
    "- **Skipgram**:\n",
    "  - **Positivos**: Captura el simbolismo Targaryen (\"threeheaded\" 0.62, \"stormborn\" 0.50) y nombres de dragones (\"drogon\" 0.56) y figuras históricas (\"visenya\", \"aegon\" ~0.49-0.51).\n",
    "  - **Negativos**: Términos irrelevantes como \"greyfaced\", \"mudge\" (<0.10).\n",
    "  - **Patrones**: Enfoca la mitología Targaryen y términos específicos de dragones.\n",
    "- **CBOW**:\n",
    "  - **Positivos**: Incluye dragones (\"dragons\" 0.54, \"drogon\" 0.41, \"meraxes\" 0.39, \"balerion\" 0.38) y temas de conquista (\"conqueror\" 0.43, \"targaryen\" 0.39), pero con ruido como \"bitch\" (0.41).\n",
    "  - **Negativos**: Términos genéricos como \"terrance\", \"scowled\" (~0.36-0.40), menos informativos.\n",
    "  - **Patrones**: Captura temas de conquista y linaje, pero con menor especificidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b01479a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Palabra consultada: throne\n",
      "\n",
      "\\Modelo utilizado: Skipgram\n",
      "\n",
      "  Positivo  Similitud  Negativo  Similitud\n",
      "      iron   0.566498    popped   0.116494\n",
      "     crown   0.549585    emrick   0.108473\n",
      "     chair   0.540312     arron   0.091908\n",
      "birthright   0.530507     tells   0.082881\n",
      " fidgeting   0.517336      rast   0.071709\n",
      "rightfully   0.509013  recruits   0.071672\n",
      "   derives   0.501733 direction   0.070048\n",
      "     hinge   0.501473    mounds   0.068932\n",
      "  rightful   0.498886      bill   0.068045\n",
      "   joffrey   0.494009     fever   0.062494\n",
      "\n",
      "Palabra consultada: throne\n",
      "\n",
      "\\Modelo utilizado: CBOW\n",
      "\n",
      "  Positivo  Similitud   Negativo  Similitud\n",
      "     chair   0.526427       lice   0.388044\n",
      "     crown   0.492496    buttery   0.358328\n",
      "   victory   0.480844      piney   0.358166\n",
      "      holt   0.453929      shoot   0.357907\n",
      "      seat   0.452341     combed   0.352613\n",
      "   thrones   0.439575 disturbing   0.349671\n",
      "  damnable   0.437091      watty   0.346314\n",
      "   sconces   0.413989    weaving   0.339432\n",
      "birthright   0.409985      hates   0.338409\n",
      "     studs   0.407812  stammered   0.338306\n"
     ]
    }
   ],
   "source": [
    "mostrar_similares(w2v_model_sg, \"throne\", 10, \"Skipgram\")\n",
    "mostrar_similares(w2v_model_cbow, \"throne\", 10, \"CBOW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71306a47",
   "metadata": {},
   "source": [
    "#### Similitud con palaba **throne**\n",
    "- **Skipgram**:\n",
    "  - **Positivos**: Destaca el simbolismo del Trono de Hierro (\"iron\" 0.55, \"chair\" 0.54, \"crown\" 0.52) y términos de legitimidad (\"rightfully\", \"birthright\" ~0.49-0.50).\n",
    "  - **Negativos**: Términos irrelevantes como \"messages\", \"pies\" (<0.11).\n",
    "  - **Patrones**: Enfoca la conexión física y simbólica del trono con el poder.\n",
    "- **CBOW**:\n",
    "  - **Positivos**: Captura sinónimos (\"crown\" 0.50, \"chair\" 0.48, \"seat\" 0.44) y temas de conquista (\"victory\" 0.46, \"birthright\" 0.41), pero incluye ruido como \"sconces\" (0.41).\n",
    "  - **Negativos**: Términos genéricos como \"combed\", \"stammered\" (~0.33-0.36), menos específicos.\n",
    "  - **Patrones**: Enfatiza temas amplios de realeza y conquista, pero con menor precisión."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a66edf",
   "metadata": {},
   "source": [
    "## Diferencias entre Skipgram y CBOW\n",
    "\n",
    "1. **Especificidad vs. Generalización**:\n",
    "   - **Skipgram**: Sobresale en capturar relaciones específicas y términos raros, como nombres propios (\"qhorin\", \"ygritte\", \"visenya\"), rasgos distintivos (\"dwarf\"), y símbolos precisos (\"threeheaded\", \"iron\"). Esto lo hace ideal para contextos narrativos únicos y personajes o conceptos con características distintivas en el corpus.\n",
    "   - **CBOW**: Tiende a generalizar, capturando temas más amplios (e.g., \"father\", \"victory\") y conexiones narrativas extensas (e.g., \"davos\", \"greyjoy\"). Sin embargo, incluye términos menos relevantes (e.g., \"bitch\", \"sconces\") y negativos más genéricos (e.g., \"called\", \"grasses\"), lo que reduce su precisión para detalles específicos.\n",
    "\n",
    "2. **Similitudes en Positivos**:\n",
    "   - **Skipgram**: Produce similitudes ligeramente más altas para términos muy específicos (e.g., \"cersei\" 0.74 para Tyrion, \"threeheaded\" 0.62 para dragon) y prioriza relaciones cercanas (familia, lugares, rasgos).\n",
    "   - **CBOW**: Genera similitudes más altas para relaciones familiares o temáticas amplias (e.g., \"jaime\" 0.78 para Tyrion, \"starks\" 0.69 para stark), pero incluye términos menos relevantes, indicando una captura más general del contexto.\n",
    "\n",
    "3. **Calidad de Negativos**:\n",
    "   - **Skipgram**: Los negativos son más específicos e irrelevantes (e.g., \"fishing\", \"sceptre\" <0.13), lo que demuestra una mejor diferenciación de contextos no relacionados.\n",
    "   - **CBOW**: Los negativos son más genéricos (e.g., \"called\", \"quailed\" ~0.33-0.45), lo que refleja su tendencia a asociar términos comunes en diálogos o descripciones, reduciendo la claridad del análisis.\n",
    "\n",
    "4. **Robustez ante Ruido**:\n",
    "   - **Skipgram**: Menos propenso a incluir términos irrelevantes, pero afectado por artefactos de preprocesamiento (e.g., “aegon”, “jon”).\n",
    "   - **CBOW**: Más propenso a ruido (e.g., \"bitch\" para dragon, \"sconces\" para throne), lo que sugiere que un umbral de frecuencia mayor (e.g., `min_count=10`) podría mejorar los resultados.\n",
    "\n",
    "5. **Contexto Narrativo**:\n",
    "   - **Skipgram**: Captura mejor el \"sabor\" de la narrativa, destacando personajes secundarios (e.g., \"qhorin\", \"leobalds\") y detalles simbólicos (e.g., \"threeheaded\", \"iron\"), ideales para análisis de arcos específicos.\n",
    "   - **CBOW**: Mejor para temas generales, como liderazgo (\"victory\"), familia (\"father\"), y rivalidades (\"lions\"), pero menos efectivo para detalles específicos.\n",
    "\n",
    "---\n",
    "\n",
    "#### Observaciones Generales\n",
    "- **Patrones Narrativos Comunes**:\n",
    "  - Ambos modelos capturan relaciones familiares (Lannister para \"tyrion\", Stark para \"jon\" y \"stark\"), geográficas (\"winterfell\" para stark), mitológicas (\"drogon\", \"targaryen\" para dragon), y simbólicas (\"iron\", \"crown\" para throne).\n",
    "  - Los resultados reflejan la narrativa de *A Song of Ice and Fire*: conflictos familiares (Stark vs. Lannister), poder (trono, dragones), y lealtades (Jon con la Guardia).\n",
    "\n",
    "\n",
    "**Conclusión**: Skipgram es superior para capturar detalles específicos y términos raros. CBOW destaca en temas generales y conexiones narrativas amplias, pero es menos preciso debido a su tendencia a incluir ruido. Ambos modelos son complementarios para analizar la semántica de *A Song of Ice and Fire*, con Skipgram ofreciendo mayor precisión y CBOW mayor cobertura temática."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f454e2b",
   "metadata": {},
   "source": [
    "### Visualizar agrupación de vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f871149",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA    \n",
    "from sklearn.manifold import TSNE                   \n",
    "import numpy as np                                  \n",
    "\n",
    "def reduce_dimensions(model, num_dimensions = 2 ):\n",
    "     \n",
    "    vectors = np.asarray(model.wv.vectors)\n",
    "    labels = np.asarray(model.wv.index_to_key)  \n",
    "\n",
    "    tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
    "    vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "    return vectors, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7074804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar los embedddings en 2D\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "vecs, labels = reduce_dimensions(w2v_model_sg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00ed626a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html>\n",
       "<head><meta charset=\"utf-8\" /></head>\n",
       "<body>\n",
       "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-3.1.0.min.js\" integrity=\"sha256-Ei4740bWZhaUTQuD6q9yQlgVCMPBz6CZWhevDYPv93A=\" crossorigin=\"anonymous\"></script>                <div id=\"b530ec04-070f-4b11-8b72-73fb05ae360d\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById(\"b530ec04-070f-4b11-8b72-73fb05ae360d\")) {                    Plotly.newPlot(                        \"b530ec04-070f-4b11-8b72-73fb05ae360d\",                        [{\"hovertemplate\":\"x=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003etext=%{text}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"text\":[\"\\u0094\",\"said\",\"lord\",\"would\",\"one\",\"ser\",\"could\",\"man\",\"back\",\"men\",\"well\",\"like\",\"jon\",\"old\",\"even\",\"hand\",\"never\",\"king\",\"know\",\"see\",\"made\",\"tyrion\",\"eyes\",\"told\",\"thought\",\"black\",\"time\",\"long\",\"lady\",\"might\",\"us\",\"come\",\"father\",\"still\",\"face\",\"head\",\"way\",\"must\",\"red\",\"boy\",\"little\",\"took\",\"came\",\"good\",\"two\",\"\\u0093i\",\"though\",\"say\",\"away\",\"brother\",\"dead\",\"take\",\"son\",\"blood\",\"make\",\"go\",\"saw\",\"queen\",\"arya\",\"three\",\"first\",\"day\",\"want\",\"night\",\"look\",\"much\",\"enough\",\"white\",\"looked\",\"sword\",\"jaime\",\"knew\",\"asked\",\"gave\",\"great\",\"called\",\"left\",\"tell\",\"every\",\"girl\",\"heard\",\"went\",\"turned\",\"need\",\"behind\",\"yet\",\"wall\",\"bran\",\"around\",\"half\",\"dany\",\"beneath\",\"across\",\"another\",\"sansa\",\"let\",\"maester\",\"found\",\"keep\",\"last\",\"hands\",\"gods\",\"knight\",\"think\",\"snow\",\"feet\",\"hair\",\"castle\",\"woman\",\"many\",\"grace\",\"gold\",\"seemed\",\"cersei\",\"ever\",\"stannis\",\"stark\",\"\\u0093the\",\"may\",\"kings\",\"find\",\"done\",\"catelyn\",\"hear\",\"name\",\"lannister\",\"put\",\"prince\",\"upon\",\"stone\",\"high\",\"wine\",\"water\",\"horse\",\"voice\",\"fire\",\"gone\",\"iron\",\"hard\",\"robb\",\"seen\",\"always\",\"years\",\"better\",\"give\",\"shall\",\"place\",\"mother\",\"dark\",\"small\",\"grey\",\"stood\",\"cold\",\"end\",\"hundred\",\"sam\",\"ned\",\"robert\",\"words\",\"right\",\"winterfell\",\"brothers\",\"walls\",\"nothing\",\"fingers\",\"cloak\",\"beside\",\"house\",\"\\u0093you\",\"sea\",\"young\",\"mouth\",\"door\",\"watch\",\"get\",\"sent\",\"big\",\"wanted\",\"almost\",\"sister\",\"others\",\"true\",\"leave\",\"lost\",\"felt\",\"perhaps\",\"light\",\"wind\",\"city\",\"dont\",\"daughter\",\"seven\",\"sweet\",\"children\",\"side\",\"ill\",\"brought\",\"lords\",\"green\",\"died\"],\"x\":{\"dtype\":\"f4\",\"bdata\":\"Fb9QwY9AMMF\\u002fI5jAWBPCwH3Gbb5w0AFC\\u002f7iwwHSVicBmLLBAFbqLQNaFbr\\u002f9TYm\\u002fhrCDweOzZL\\u002faIRrAng8HQUqpjMD4nAfBoWQYwbQqJzxkYw+\\u002fwlifwWNyycGQxwHBGW6dwNlnZEHVNv0\\u002fRfsSvz7k0EGbAb7AH4E8wSZRL0BSB57Ay27cv5afnEELSahBTTawQJ5oacEvYlNB\\u002fcKqwEDteL\\u002flHjBAfD8+QH2F08CE4eNAC+NJwSftGcDL4g3BxuGrQIohS8Bw2wHAviMAQNbT+EH6oghA2iIGv9yXTUDpqkY+BhGTwUNGwMBK7e9A2vOHP6JhHkBYMGLBXVUZQNA+csArKHHA8Rc5wNsJYUFcRYDAZ4S7QS0WnsF9HpPAoM0swc3NGD8xyAxBlvDjQQv3j0BIdQjB2SDFwe4QqMBz+RLBWfa2QCsEvz9wGGHBGCe0QCkgEsBFaUlBXIjMwKCNrUFhro89fFmuwarYJcA09fLB4LzCvoeewMB1pnPB4IKywS1bjD8U15hAh6OKP2feDUGVBvrASbOkQBbRAcFz5gFBHguQQC7mj0HklwVBzV+RwHD6f8HavGXBTKs0QXJ0WcAzPp3BqOqDwDia38Bdp6vA6u8mwUMgcsHx1QTArzWcP9SrnMAp1cPASzkTwXyZwMDuLtPAr8rqPzXN1MBjQIvBsZJ1QRUJn8EVCo1BATudQS+WyUEvWbw+PnVGQV1bCUC+C\\u002f7BH\\u002fD\\u002fQBokksDMDJw93Fx8wOJOf0CGbJXAwjCcP3gsc8HvdRG\\u002fiTSAwO1RFkEzotrADdZnQWxeckFoN1dBFxEcQE8NnkDW0XnBd+mhwPgW+cBD9HHBgKOdQHKphsAoTzXAxN5bQeYrTMENtQxBFyh8QfiOukBVRc5BONZJwQ+lRkFbM6ZB9A+4Pg95AMJ7StPBNPHBP9KNRkAZ5pxB6NZcwdLqb0Aq0H\\u002fAqu97QNUiu0H2MYFAuH15QDGmNr3erZvAdKkxQe84KEGjav2+c3GDwWSA40GooAnB7R8ewbZVhUCZEL1AMSeAwUAPN0B6CAFCYLNyQWISyb8=\"},\"xaxis\":\"x\",\"y\":{\"dtype\":\"f4\",\"bdata\":\"mEwJQUrO1EDnoCzBZguMwLHAIMBV09TAEO5FwG0zuj+6\\u002f71AMxYRwQS3ZsDazRa\\u002fnXKUQVWBKMEZyC7AeJE6QXUPZsBS6Y3BirqSwK+4tj+v+E0\\u002fXHGRQIpKO8HzFou\\u002fVINov6jUhMG5ehDAGzVXwDOmHMHfWY3AeRKswIBMAr9ZC03BkPgiwIi6MMEVHp\\u002fARChLv5e7rcCnbVHBeBoBQG6WJ8Cp2BhAdBQ5vvnsFsBX5h7BilffPxYPFMDieFbAODTAQJp7VcHl8GDBaF20P7GrO8ET897B3tgWP7WDub1wIhdAuHN+QKXxy0CE4CPBOX81wN94O8AbjY\\u002fAyoUbwLerHUFpMVvAhJ\\u002fIv\\u002fb0WcFoIiJBlWWLwQoEikAUQRnAflHZQHCRBz\\u002favIvBQ9VlQRcaW0ABy0TAGcOmv7sk\\u002fT9tx4lB7hyaQIt7NkHKeJLAXJ9sQEU3ecDwU+a9TB\\u002fLQJ1QF0FWJ0\\u002fAromWwREA5sHLPjc\\u002f2n4KwLdtykCEEFHAdSYNQew\\u002fVkCg5FU\\u002fivgywFwbSkGI+WjBd1Y6wRdsdsCHBYQ\\u002f1wDqQHGMc8FjVBTAg4nsPxXcV8Ht\\u002f9vALrCXwUrgH8BsPIlAuJpnwOyGlMFodXvBrGFcQA9PqsCs9o\\u002fBZ82nP7ryosBb\\u002fcdAMPeJQcN6RD\\u002fwf2PBwLwaQATWqcEhc1rBab0rQDqwGEFuwqdBy1HEQO31N0DpJKNBaFASQcNobUDUDLC+Iq0iQaXxc8EBpQdAcfBGwPnpo8Cf9G\\u002fAsIUsP5sbq8D\\u002fBfDAtlpPwc0u8z+Q8nhAlf1MwQbpFUFoyHVBLrCcwPNKt8FWl5FB0AR1wYLZmsGXjDJBNBFhQBFCg8FFYFTBnoOCPgczbMCZmD9BwUPFwaySbkBt\\u002fj7BKknnP+rHFsDJVwXBAF4bQdJwxUBo+pfAKz\\u002f4P6Xxsz\\u002frKpPAFgiPwFlAA0Gu5VDBDcAHwdftSsAJU6w+WfJlQCw7p0AHXJrAZQbfQOlyqUGFzL\\u002fBOd2GwMJwTsE8scLBU2InQNsREsFw3mVAfO2SwAdBrT99u8Y+dL6KwQnRacE=\"},\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermap\":[{\"type\":\"scattermap\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"x\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"y\"}},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('b530ec04-070f-4b11-8b72-73fb05ae360d');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };            </script>        </div>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MAX_WORDS=200\n",
    "fig = px.scatter(x=vecs[:MAX_WORDS,0], y=vecs[:MAX_WORDS,1], text=labels[:MAX_WORDS])\n",
    "fig.show(renderer=\"colab\") # esto para plotly en colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15a47f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs_3d, labels_3d = reduce_dimensions(w2v_model_sg, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c622bb55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html>\n",
       "<head><meta charset=\"utf-8\" /></head>\n",
       "<body>\n",
       "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-3.1.0.min.js\" integrity=\"sha256-Ei4740bWZhaUTQuD6q9yQlgVCMPBz6CZWhevDYPv93A=\" crossorigin=\"anonymous\"></script>                <div id=\"4041f6e4-4465-4583-8a63-d58966bc87f2\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById(\"4041f6e4-4465-4583-8a63-d58966bc87f2\")) {                    Plotly.newPlot(                        \"4041f6e4-4465-4583-8a63-d58966bc87f2\",                        [{\"hovertemplate\":\"x=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003ez=%{z}\\u003cbr\\u003etext=%{text}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\",\"size\":2},\"mode\":\"markers+text\",\"name\":\"\",\"scene\":\"scene\",\"showlegend\":false,\"text\":[\"\\u0094\",\"said\",\"lord\",\"would\",\"one\",\"ser\",\"could\",\"man\",\"back\",\"men\",\"well\",\"like\",\"jon\",\"old\",\"even\",\"hand\",\"never\",\"king\",\"know\",\"see\",\"made\",\"tyrion\",\"eyes\",\"told\",\"thought\",\"black\",\"time\",\"long\",\"lady\",\"might\",\"us\",\"come\",\"father\",\"still\",\"face\",\"head\",\"way\",\"must\",\"red\",\"boy\",\"little\",\"took\",\"came\",\"good\",\"two\",\"\\u0093i\",\"though\",\"say\",\"away\",\"brother\",\"dead\",\"take\",\"son\",\"blood\",\"make\",\"go\",\"saw\",\"queen\",\"arya\",\"three\",\"first\",\"day\",\"want\",\"night\",\"look\",\"much\",\"enough\",\"white\",\"looked\",\"sword\",\"jaime\",\"knew\",\"asked\",\"gave\",\"great\",\"called\",\"left\",\"tell\",\"every\",\"girl\",\"heard\",\"went\",\"turned\",\"need\",\"behind\",\"yet\",\"wall\",\"bran\",\"around\",\"half\",\"dany\",\"beneath\",\"across\",\"another\",\"sansa\",\"let\",\"maester\",\"found\",\"keep\",\"last\",\"hands\",\"gods\",\"knight\",\"think\",\"snow\",\"feet\",\"hair\",\"castle\",\"woman\",\"many\",\"grace\",\"gold\",\"seemed\",\"cersei\",\"ever\",\"stannis\",\"stark\",\"\\u0093the\",\"may\",\"kings\",\"find\",\"done\",\"catelyn\",\"hear\",\"name\",\"lannister\",\"put\",\"prince\",\"upon\",\"stone\",\"high\",\"wine\",\"water\",\"horse\",\"voice\",\"fire\",\"gone\",\"iron\",\"hard\",\"robb\",\"seen\",\"always\",\"years\",\"better\",\"give\",\"shall\",\"place\",\"mother\",\"dark\",\"small\",\"grey\",\"stood\",\"cold\",\"end\",\"hundred\",\"sam\",\"ned\",\"robert\",\"words\",\"right\",\"winterfell\",\"brothers\",\"walls\",\"nothing\",\"fingers\",\"cloak\",\"beside\",\"house\",\"\\u0093you\",\"sea\",\"young\",\"mouth\",\"door\",\"watch\",\"get\",\"sent\",\"big\",\"wanted\",\"almost\",\"sister\",\"others\",\"true\",\"leave\",\"lost\",\"felt\",\"perhaps\",\"light\",\"wind\",\"city\",\"dont\",\"daughter\",\"seven\",\"sweet\",\"children\",\"side\",\"ill\",\"brought\",\"lords\",\"green\",\"died\"],\"x\":{\"dtype\":\"f4\",\"bdata\":\"YVXgQQlwp0HMI\\u002ffAeft6QCQXxkP9Yh7ApCqNQK\\u002f9gUH1LsI\\u002fDAPKwZ7KRkRGaTfBOW0UQij2pUHQDXpAoHQNwPKNAUGEHo5BAIcfQVIVc8EI6AbBZ7UHQkG6X0EheclAyO2wQVWwPMH7qKJA\\u002fT0Wwbjxk0El64JA8XImwY0PnsHYb55B3giaQEuckEBlLILBUdJfwW64nEDgZRLBW+HJQYLZEEJo0otBx7B\\u002fQXfIzsCIiS1BVI0wQodcJEMak3lAa0WnPwjZiUHTeA9BhX6vwUY1CsFUQyk\\u002f0HD\\u002fwLdws8GD5bJB\\u002fRnLQThmDULh1zdBFwEZQQM1FT72h\\u002fU\\u002fE1wdQFQgGUJZlJ7AFMaMwSRbNEBdLxJCfOAAwf80BELKX3pB0ucnQhqCXUKAVsPBaIW1P2x0KkFI4CNAXe8bQgJC0EG6WTpCSPiRQcV2ykFq39Y9Zfg8QKFTJ8OMFN7BZPUMQvqRM8EZIy\\u002fCWRBAQu0+vEGnSvdBBabzQe5ABkKJ2V9COHM7wRcaikHZoabAltQpQcmZEMA8B4pB4JSrwYKvuj+yaflBxZcrPysH+T+TyZvBjq2QQb\\u002fch0EtYLdA1F6Fv6kTBcCtE\\u002f5BZjDzQL2sUkGfQ99BmLMIQv8hvkCTGGJAlECBwVryIUGrVAFCDHk\\u002fQiWXfkKyxONBf7WawDPxdUEsTMe+ZhWMwRQs9kFHVgnCGU02wqnZj8FGbDhCDIERwWPJFUF6d4TB9qRlP1Mp4UELla1BLUkiQVGd40D8YMK+Ml+Vwc8XmUDSby\\u002fB87bDQaxoV0BOhw9CJhzWwJjycEFvGYPBEIpjwfPEDUEj8iNCHcvsQVe6s0E\\u002f2BRC1VmyQJzNar1bIplBjcDdwcZlx0HSDJvALC6lwUe3gT8mrsVAo6QsQmEYPsJtv4pBkaEyQeqHdsCRMIVApv6UwSY1gsGtDmvCH3zKP7sbHkGMfMRByFqewWo1TkJn58bBL95EQSChzrw8ejbBC3TxPwKhIEFwChzC9QQ8QQAtN0Gu99pBuOUqQlmI08HB3SbAy40SQZZVa8G0CRXC5W9SQJw7CME=\"},\"y\":{\"dtype\":\"f4\",\"bdata\":\"lW8iQgB\\u002f+kH8TudBt5wgQSMMu0N4Xx5CJ0eNQIN8g8BOZL\\u002fBACYfQNA2DkMol4fAEFmVQbyqA7+O6aC\\u002fpzZDwXlTlkD\\u002fwZ1B3efBQfadyUDjD94\\u002f2TatQcp3dcFANhdC7vWzvzuPPcK\\u002f1WfBvk5MwQdS30EGNCtBm+OZQWDP\\u002fEDboDZBlMaVwM4KYMGSQRrA3ACZwKpjKUGYt4HCFuQtQcmFs8FB+ZDBsv4CwunTkUEEmbPBRm63QftIGkMTxgFCr529wXvr5EADwjvBw1SoQb5vlEH4xfLBLbImQNYqOEFtC93BtDPRQXWiW0EDlKfBGPIHwamyT8E3RHhB3hWKwVdJz8Fc+9NBwziiQaJRdcIJAsDB65alwfhvukEGHRO\\u002fFAH2QE2jKEC6tY\\u002fBsEsYQif4ucGYqQBChwENwJTDH0GRfZnBn6kAws+K9MFAoXlB8R2vwTNsncKP7cTBmZdhQWxwtMGkOs1BoXHdQStMScKZED3CoxgHwc8NU0EhpLPAX6dLQpf0yMFNJ6C\\u002f6f8cwdymVsFEv8nAnw4xQcWUw0GE+9fBLUxEwmYLIMJ8uTPBee5KwN9qwsFSPShCNaN2wsxn8D+IralBkw4+QFwvfUHBwx5BFwkPQqUxQUFkuWdBmA7HQca\\u002fzECIhllBqKmhweKNyMDa1JlB8OkWwGcAzkHLh6HB3HHhwezOGkBQ7Q1BwDafwRdKE8GZ6JzBKu9Swi8ggMGC+oTBxISjv6M7L0E+VefBKxb4QH+vLcFBl19BtpDSQWw3VEHMtR\\u002fByPnTQPxmJ8Kkqs7BBlVJwapsCsII3RLBd9UewVNztMFUULBBGpRuQSOhjUFR1zlCCC3AwVr2GUEt7Z1AL\\u002f3TwdVHr0DPPznBafBpwocnvsFRizhBwzOuQU4UasHqPY9AcsSGwW4hDcIsFwrB2lhtQRQxp0G5bX3BiyNjQfuECMJo4QlB15YMQBaI2MCMFspBCn7Zwfa7GUDbH4VBlMhMwlfLKsLUO65A9qxcQT1mdUHtXZbAneuLv4JPjED478rB8oFBQWx7s0FCXfVBgx1WwnXmB0E=\"},\"z\":{\"dtype\":\"f4\",\"bdata\":\"nftlQKl3X0HjGqrBQv8uQbDmGMM\\u002fkeXB1FE9QTZvZUElfuTAZfh8wcGwgUMkH4RBLZjpwcMLHsKwVIVAuCXJQRWdvT+hK4TBoHWkQcbcgUGwhYJB0wiVwdJPHkJpnZ5BuBt6QMTbnr\\u002fXygLCr2Y8wlleM8IvGxZBIYihQMJHMEDqxK\\u002fB99a4QBgGXUIW9s1Brc+nwANVUkGEihHBdZs6waqlNULYQ4TBUisWwYVJY0FSKSjCEN6PwFWIP0Pn+t9BOluQwInBn8Fn7O++IESJQYXITsJWgDlCQ3ptQQnPWUE5ot7BqQ2LwURXrMHIMi3Cee0bwq0lHsKDYJJBCNIPwqsoyz8kvwdCyiEJQrwQmsDKSw9A8v9BQQ4On8Eu43I8pvhjQcYtRsFErRPBVvP+QbYNicEwkctB0FpawsOvHcHETURAEvgowb8aFcBFgpFBD32RwV\\u002fxPsP4s73BQijKwTaglb4waGpBuNs4wZqRbEEfOYlBJpZzwXDjp8HVPBTAJS9WwVjDr8HC3to\\u002fZ3gZwpoK4EGpU7TBOcOvwZhh3EFFsQXCwokrwiUiAkKfWAfC63VYQcKvMcKIdIhAUYWtQX\\u002fxT0HcWZnBNUIbQAzpncFIAQzC09MPwcMlKEEuk77BHbGjQRXS2cC2Y7zBngUZQB0AOr9FD3bBnntoQfDb2sHufqPBj2UzwYG3isFmeBdCMPWJQRTXM8EQMaxBtvqDwWKbc8F7\\u002f59BkJ8TQhAr4cFjkezB1l1iP4skNMJvlRdBRftQQZS2TEEAZeXB2D+QwRX2zsHSHD5CkDtqwXg6o8FH1Z5Bu6Iowh\\u002fHRMKwpdPBsyjbwTVWsMG2iu5B3XRuwYrz58He4KjBKRjTwcdJ1kGiN8lB2AbSQTdIl8HbN1TCvCOxwKoI3cHjvSPCV+WyQVn8dkBqwUnCmyGgQeNdMEG4\\u002f0hBYZJ\\u002fQUyQFcGgEJnBEUc9wbZPIcEnCuhAjAGHwZoWXEJgXCNBrI+SwfFaRsE6\\u002fYHBjnWZQTURNMLkLq\\u002fB8tw5QMP8ZsGKtYbBupaAQY4aWEFQPp3BH8S0QZs92cE=\"},\"type\":\"scatter3d\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermap\":[{\"type\":\"scattermap\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"scene\":{\"domain\":{\"x\":[0.0,1.0],\"y\":[0.0,1.0]},\"xaxis\":{\"title\":{\"text\":\"x\"}},\"yaxis\":{\"title\":{\"text\":\"y\"}},\"zaxis\":{\"title\":{\"text\":\"z\"}}},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('4041f6e4-4465-4583-8a63-d58966bc87f2');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };            </script>        </div>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = px.scatter_3d(x=vecs_3d[:MAX_WORDS,0], y=vecs_3d[:MAX_WORDS,1], z=vecs_3d[:MAX_WORDS,2],text=labels_3d[:MAX_WORDS])\n",
    "fig.update_traces(marker_size = 2)\n",
    "fig.show(renderer=\"colab\") # esto para plotly en colab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c11259",
   "metadata": {},
   "source": [
    "### Análisis del gráfico\n",
    "Se observa que en 3D compacta la gran mayoría de las palabras en una región, con algunas palabras sueltas, por lo que analizamos sobre el gráfico 2D que provee mayor claridad. Para facilitar el análisis, tomamos capturas del gráfico:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449f4872",
   "metadata": {},
   "source": [
    "<img src=\"Capturas T-SNE/Captura_1.png\" alt=\"imagen1\" style=\"width: 100%;\">\n",
    "\n",
    "Se observa una relación entre familiares que son Tyrion, Jaime y Cersei, observada previamente en el análisis de similitud. También Cersei es la más cercana a la palabra \"queen\", siendo que ella se convierte en reina durante la saga."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8363ffc",
   "metadata": {},
   "source": [
    "<img src=\"Capturas T-SNE/Captura_2.png\" alt=\"imagen1\" style=\"width: 100%;\">\n",
    "\n",
    "Se observa una relación entre familiares (Stannis y Robert), y la posición que ocupan (Robert es rey, Stannis principe y aspirante a rey)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e3b7d3",
   "metadata": {},
   "source": [
    "<img src=\"Capturas T-SNE/Captura_3.png\" alt=\"imagen1\" style=\"width: 100%;\">\n",
    "\n",
    "Se observa que logra captar relaciones entre colores, además de que aparece la palabra \"hair\" como cercana"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb03160",
   "metadata": {},
   "source": [
    "<img src=\"Capturas T-SNE/Captura_4.png\" alt=\"imagen1\" style=\"width: 100%;\">\n",
    "\n",
    "Logra identificar como cercanas palabras de relaciones familiares como \"hija\", \"hijo\", como \"casa\" (en el sentido de familia)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe63b4fd",
   "metadata": {},
   "source": [
    "<img src=\"Capturas T-SNE/Captura_5.png\" alt=\"imagen1\" style=\"width: 100%;\">\n",
    "\n",
    "Identifica los números 2 y 3 como cercanos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b8b3a9",
   "metadata": {},
   "source": [
    "<img src=\"Capturas T-SNE/Captura_6.png\" alt=\"imagen1\" style=\"width: 100%;\">\n",
    "\n",
    "Identifica como cercanos palabras como \"stark\", \"ned\" (miembro de esa familia), \"robb\" (hijo de Ned), \"winterfell\" (hogar de los Stark), y también aparece como cercana \"lannister\" (que es la familia en conflicto con los Stark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618e251c",
   "metadata": {},
   "source": [
    "<img src=\"Capturas T-SNE/Captura_7.png\" alt=\"imagen1\" style=\"width: 100%;\">\n",
    "\n",
    "Las facciones de los \"other\", los hombres (\"men\"), y \"children\" (en referencia a Children of the Forest), estuvieron en guerra en las leyendas narradas en las novelas, por lo que las identifica como cercanas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7490ec0d",
   "metadata": {},
   "source": [
    "### Conclusiones\n",
    "1. Es fundamental el preprocesamiento de texto para entrenar los modelos. Si bien se eliminaron acentos, stopwords, y caracteres especiales, al momento de capturar las similitudes aparecieron algunos caracteres restantes para limpiar.\n",
    "2. Skipgram captura relaciones específicas y términos raros (e.g., \"qhorin\", \"threeheaded\") al predecir el contexto a partir de una palabra central, siendo ideal para detalles narrativos, pero sensible a artefactos de preprocesamiento. CBOW generaliza temas amplios (e.g., \"father\", \"victory\") al predecir una palabra a partir de su contexto, pero incluye más ruido (e.g., \"bitch\") y negativos genéricos, requiriendo un filtrado más estricto.\n",
    "3. Los modelos en general capturaron como mayor similitud las relaciones familiares (por ejemplo \"Tyrion\", \"Jaime\" y \"Cersei\", o \"Ned\" y \"Robb\").\n",
    "4. Aparecieron también adjetivos muy relacionados a un determinado personaje, por ejemplo la palabra \"enano\" (\"dwarf\") con \"Tyrion\".\n",
    "5. En el caso de palabras más generales como \"throne\" o \"dragon\", logró capturar tanto su relación con personajes, como su pertenencia a símbolos de casas, o a otros conceptos relacionados (por ejemplo \"birthright\" o \"rightfully\" con \"throne\")\n",
    "6. Por último, también logra capturar palabras similares por fuera del contexto de la novela (en los ejemplos mostrados, aparecieron colores y números)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
