{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3yeJGnCYxuF"
      },
      "source": [
        "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
        "\n",
        "\n",
        "# Procesamiento de lenguaje natural\n",
        "## Modelo de lenguaje con tokenización por caracteres"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iv5PEwGzZA9-"
      },
      "source": [
        "### Consigna\n",
        "- Seleccionar un corpus de texto sobre el cual entrenar el modelo de lenguaje.\n",
        "- Realizar el pre-procesamiento adecuado para tokenizar el corpus, estructurar el dataset y separar entre datos de entrenamiento y validación.\n",
        "- Proponer arquitecturas de redes neuronales basadas en unidades recurrentes para implementar un modelo de lenguaje.\n",
        "- Con el o los modelos que consideren adecuados, generar nuevas secuencias a partir de secuencias de contexto con las estrategias de greedy search y beam search determístico y estocástico. En este último caso observar el efecto de la temperatura en la generación de secuencias.\n",
        "\n",
        "\n",
        "### Sugerencias\n",
        "- Durante el entrenamiento, guiarse por el descenso de la perplejidad en los datos de validación para finalizar el entrenamiento. Para ello se provee un callback.\n",
        "- Explorar utilizar SimpleRNN (celda de Elman), LSTM y GRU.\n",
        "- rmsprop es el optimizador recomendado para la buena convergencia. No obstante se pueden explorar otros.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Y-QdFbHZYj7C"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-28 19:39:28.564534: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-09-28 19:39:28.564857: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-09-28 19:39:28.609000: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-09-28 19:39:29.763030: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-09-28 19:39:29.763380: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import io\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding, Dropout\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTvXlEKQZdqx"
      },
      "source": [
        "### Datos\n",
        "Como fuente de datos de utilizarán las novelas de \"A Song of Ice and Fire\" de George Martin, utilizadas durante el Desafío 2. Las mismas fueron obtenidas de Kaggle:\n",
        "https://www.kaggle.com/datasets/saurabhbadole/game-of-thrones-book-dataset/data\n",
        "\n",
        "El dataset cotiene cinco archivos de texto, cada uno con un libro de la saga. Se comienza concatenando todos los archivos en un solo corpus:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7amy6uUaBLVD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "lista_libros = [\n",
        "    \"1 - A Game of Thrones.txt\"]\n",
        "\n",
        "# Crear un corpus en memoria\n",
        "corpus = \"\"\n",
        "\n",
        "for nombre in lista_libros:\n",
        "    ruta_libro = os.path.join(\"./Dataset\", nombre)\n",
        "    with open(ruta_libro, \"r\", encoding=\"latin-1\") as f:\n",
        "        contenido = f.read()\n",
        "        corpus += contenido + \"\\n\"  # agrega un salto de línea entre libros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Removemos los encabezados que no forman parte de las novelas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re \n",
        "\n",
        "#Eliminar encabezados no deseados\n",
        "import re\n",
        "\n",
        "patterns_to_remove = [\n",
        "    r\"a game of thrones\",\n",
        "    r\"a clash of kings\",\n",
        "    r\"a storm of swords\",\n",
        "    r\"a feast for crows\",\n",
        "    r\"a dance with dragons\",\n",
        "    r\"book [^\\n]+\",          # líneas tipo \"Book One of...\"\n",
        "    r\"by george r\\. r\\. martin\",\n",
        "    r\"prologue\",\n",
        "    r\"dedication\",\n",
        "    r\"contents\",\n",
        "    r\"a note on chronology\",\n",
        "    r\"a cavil on chronology\",\n",
        "    r\"version history.*\",\n",
        "    r\"page \\d+\",             # \"Page XX\"\n",
        "    r\"[\\n\\r\\t]+\",            # saltos de línea, retorno de carro, tabulaciones\n",
        "    r\"[^\\x20-\\x7E]+\"         # caracteres no ASCII imprimibles\n",
        "]\n",
        "\n",
        "for pat in patterns_to_remove:\n",
        "    corpus = re.sub(pat, \" \", corpus, flags=re.IGNORECASE)\n",
        "\n",
        "# Reemplazar múltiples espacios por uno solo y recortar inicio/final\n",
        "corpus = re.sub(r\"\\s+\", \" \", corpus).strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WBE0sSYuB-E6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\"We should start back,\" Gared urged as the woods began to grow dark around them. \"The wildlings are dead.\" \"Do the dead frighten you?\" Ser Waymar Royce asked with just the hint of a smile. Gared did not rise to the bait. He was an old man, past fifty, and he had seen the lordlings come and go. \"Dead is dead,\" he said. \"We have no business with the dead.\" \"Are they dead?\" Royce asked softly. \"What proof have we?\" \"Will saw them,\" Gared said. \"If he says they are dead, that\\'s proof enough for me.\" Will had known they would drag him into the quarrel sooner or later. He wished it had been later rather than sooner. \"My mother told me that dead men sing no songs,\" he put in. \"My wet nurse said the same thing, Will,\" Royce replied. \"Never believe anything you hear at a woman\\'s tit. There are things to be learned even from the dead.\" His voice echoed, too loud in the twilit forest. \"We have a long ride before us,\" Gared pointed out. \"Eight days, maybe nine. And night is falling.\" Ser Waymar Ro'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# En corpus se encuentra el texto\n",
        "corpus[:1000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP1JdiOIKQWi"
      },
      "source": [
        "### Elegir el tamaño del contexto\n",
        "\n",
        "En este caso, como el modelo de lenguaje es por caracteres, todo un gran corpus\n",
        "de texto puede ser considerado un documento en sí mismo y el tamaño de contexto\n",
        "puede ser elegido con más libertad en comparación a un modelo de lenguaje tokenizado por palabras y dividido en documentos más acotados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wumBNwdjJM3j"
      },
      "outputs": [],
      "source": [
        "# seleccionamos el tamaño de contexto\n",
        "max_context_size = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "m5FeTaGvbDbw"
      },
      "outputs": [],
      "source": [
        "# Usaremos las utilidades de procesamiento de textos y secuencias de Keras\n",
        "from tensorflow.keras.utils import pad_sequences # se utilizará para padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "573Cg5n7VhWw"
      },
      "outputs": [],
      "source": [
        "# en este caso el vocabulario es el conjunto único de caracteres que existe en todo el texto\n",
        "chars_vocab = set(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VwTK6xgLJd8q"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "77"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# la longitud de vocabulario de caracteres es:\n",
        "len(chars_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2W0AeQjXV1Ou"
      },
      "outputs": [],
      "source": [
        "# Construimos los dicionarios que asignan índices a caracteres y viceversa.\n",
        "# El diccionario `char2idx` servirá como tokenizador.\n",
        "char2idx = {k: v for v,k in enumerate(chars_vocab)}\n",
        "idx2char = {v: k for k,v in char2idx.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oIUjVU0LB0r"
      },
      "source": [
        "###  Tokenizar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "h07G3srdJppo"
      },
      "outputs": [],
      "source": [
        "# tokenizamos el texto completo\n",
        "tokenized_text = [char2idx[ch] for ch in corpus]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "PwGVSKOiJ5bj"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[11,\n",
              " 21,\n",
              " 32,\n",
              " 18,\n",
              " 2,\n",
              " 70,\n",
              " 14,\n",
              " 54,\n",
              " 34,\n",
              " 1,\n",
              " 18,\n",
              " 2,\n",
              " 45,\n",
              " 29,\n",
              " 16,\n",
              " 45,\n",
              " 18,\n",
              " 57,\n",
              " 29,\n",
              " 27,\n",
              " 71,\n",
              " 26,\n",
              " 11,\n",
              " 18,\n",
              " 40,\n",
              " 29,\n",
              " 16,\n",
              " 32,\n",
              " 1,\n",
              " 18,\n",
              " 54,\n",
              " 16,\n",
              " 50,\n",
              " 32,\n",
              " 1,\n",
              " 18,\n",
              " 29,\n",
              " 2,\n",
              " 18,\n",
              " 45,\n",
              " 70,\n",
              " 32,\n",
              " 18,\n",
              " 9,\n",
              " 14,\n",
              " 14,\n",
              " 1,\n",
              " 2,\n",
              " 18,\n",
              " 57,\n",
              " 32,\n",
              " 50,\n",
              " 29,\n",
              " 41,\n",
              " 18,\n",
              " 45,\n",
              " 14,\n",
              " 18,\n",
              " 50,\n",
              " 16,\n",
              " 14,\n",
              " 9,\n",
              " 18,\n",
              " 1,\n",
              " 29,\n",
              " 16,\n",
              " 71,\n",
              " 18,\n",
              " 29,\n",
              " 16,\n",
              " 14,\n",
              " 54,\n",
              " 41,\n",
              " 1,\n",
              " 18,\n",
              " 45,\n",
              " 70,\n",
              " 32,\n",
              " 42,\n",
              " 66,\n",
              " 18,\n",
              " 11,\n",
              " 69,\n",
              " 70,\n",
              " 32,\n",
              " 18,\n",
              " 9,\n",
              " 76,\n",
              " 34,\n",
              " 1,\n",
              " 34,\n",
              " 76,\n",
              " 41,\n",
              " 50,\n",
              " 2,\n",
              " 18,\n",
              " 29,\n",
              " 16,\n",
              " 32,\n",
              " 18,\n",
              " 1,\n",
              " 32,\n",
              " 29,\n",
              " 1,\n",
              " 66,\n",
              " 11,\n",
              " 18,\n",
              " 11,\n",
              " 33,\n",
              " 14,\n",
              " 18,\n",
              " 45,\n",
              " 70,\n",
              " 32,\n",
              " 18,\n",
              " 1,\n",
              " 32,\n",
              " 29,\n",
              " 1,\n",
              " 18,\n",
              " 73,\n",
              " 16,\n",
              " 76,\n",
              " 50,\n",
              " 70,\n",
              " 45,\n",
              " 32,\n",
              " 41,\n",
              " 18,\n",
              " 52,\n",
              " 14,\n",
              " 54,\n",
              " 67,\n",
              " 11,\n",
              " 18,\n",
              " 5,\n",
              " 32,\n",
              " 16,\n",
              " 18,\n",
              " 21,\n",
              " 29,\n",
              " 52,\n",
              " 42,\n",
              " 29,\n",
              " 16,\n",
              " 18,\n",
              " 30,\n",
              " 14,\n",
              " 52,\n",
              " 27,\n",
              " 32,\n",
              " 18,\n",
              " 29,\n",
              " 2,\n",
              " 71,\n",
              " 32,\n",
              " 1,\n",
              " 18,\n",
              " 9,\n",
              " 76,\n",
              " 45,\n",
              " 70,\n",
              " 18,\n",
              " 15,\n",
              " 54,\n",
              " 2,\n",
              " 45,\n",
              " 18,\n",
              " 45,\n",
              " 70,\n",
              " 32,\n",
              " 18,\n",
              " 70,\n",
              " 76,\n",
              " 41,\n",
              " 45,\n",
              " 18,\n",
              " 14,\n",
              " 73,\n",
              " 18,\n",
              " 29,\n",
              " 18,\n",
              " 2,\n",
              " 42,\n",
              " 76,\n",
              " 34,\n",
              " 32,\n",
              " 66,\n",
              " 18,\n",
              " 40,\n",
              " 29,\n",
              " 16,\n",
              " 32,\n",
              " 1,\n",
              " 18,\n",
              " 1,\n",
              " 76,\n",
              " 1,\n",
              " 18,\n",
              " 41,\n",
              " 14,\n",
              " 45,\n",
              " 18,\n",
              " 16,\n",
              " 76,\n",
              " 2,\n",
              " 32,\n",
              " 18,\n",
              " 45,\n",
              " 14,\n",
              " 18,\n",
              " 45,\n",
              " 70,\n",
              " 32,\n",
              " 18,\n",
              " 57,\n",
              " 29,\n",
              " 76,\n",
              " 45,\n",
              " 66,\n",
              " 18,\n",
              " 61,\n",
              " 32,\n",
              " 18,\n",
              " 9,\n",
              " 29,\n",
              " 2,\n",
              " 18,\n",
              " 29,\n",
              " 41,\n",
              " 18,\n",
              " 14,\n",
              " 34,\n",
              " 1,\n",
              " 18,\n",
              " 42,\n",
              " 29,\n",
              " 41,\n",
              " 26,\n",
              " 18,\n",
              " 8,\n",
              " 29,\n",
              " 2,\n",
              " 45,\n",
              " 18,\n",
              " 73,\n",
              " 76,\n",
              " 73,\n",
              " 45,\n",
              " 52,\n",
              " 26,\n",
              " 18,\n",
              " 29,\n",
              " 41,\n",
              " 1,\n",
              " 18,\n",
              " 70,\n",
              " 32,\n",
              " 18,\n",
              " 70,\n",
              " 29,\n",
              " 1,\n",
              " 18,\n",
              " 2,\n",
              " 32,\n",
              " 32,\n",
              " 41,\n",
              " 18,\n",
              " 45,\n",
              " 70,\n",
              " 32,\n",
              " 18,\n",
              " 34,\n",
              " 14,\n",
              " 16,\n",
              " 1,\n",
              " 34,\n",
              " 76,\n",
              " 41,\n",
              " 50,\n",
              " 2,\n",
              " 18,\n",
              " 27,\n",
              " 14,\n",
              " 42,\n",
              " 32,\n",
              " 18,\n",
              " 29,\n",
              " 41,\n",
              " 1,\n",
              " 18,\n",
              " 50,\n",
              " 14,\n",
              " 66,\n",
              " 18,\n",
              " 11,\n",
              " 33,\n",
              " 32,\n",
              " 29,\n",
              " 1,\n",
              " 18,\n",
              " 76,\n",
              " 2,\n",
              " 18,\n",
              " 1,\n",
              " 32,\n",
              " 29,\n",
              " 1,\n",
              " 26,\n",
              " 11,\n",
              " 18,\n",
              " 70,\n",
              " 32,\n",
              " 18,\n",
              " 2,\n",
              " 29,\n",
              " 76,\n",
              " 1,\n",
              " 66,\n",
              " 18,\n",
              " 11,\n",
              " 21,\n",
              " 32,\n",
              " 18,\n",
              " 70,\n",
              " 29,\n",
              " 43,\n",
              " 32,\n",
              " 18,\n",
              " 41,\n",
              " 14,\n",
              " 18,\n",
              " 57,\n",
              " 54,\n",
              " 2,\n",
              " 76,\n",
              " 41,\n",
              " 32,\n",
              " 2,\n",
              " 2,\n",
              " 18,\n",
              " 9,\n",
              " 76,\n",
              " 45,\n",
              " 70,\n",
              " 18,\n",
              " 45,\n",
              " 70,\n",
              " 32,\n",
              " 18,\n",
              " 1,\n",
              " 32,\n",
              " 29,\n",
              " 1,\n",
              " 66,\n",
              " 11,\n",
              " 18,\n",
              " 11,\n",
              " 25,\n",
              " 16,\n",
              " 32,\n",
              " 18,\n",
              " 45,\n",
              " 70,\n",
              " 32,\n",
              " 52,\n",
              " 18,\n",
              " 1,\n",
              " 32,\n",
              " 29,\n",
              " 1,\n",
              " 67,\n",
              " 11,\n",
              " 18,\n",
              " 30,\n",
              " 14,\n",
              " 52,\n",
              " 27,\n",
              " 32,\n",
              " 18,\n",
              " 29,\n",
              " 2,\n",
              " 71,\n",
              " 32,\n",
              " 1,\n",
              " 18,\n",
              " 2,\n",
              " 14,\n",
              " 73,\n",
              " 45,\n",
              " 34,\n",
              " 52,\n",
              " 66,\n",
              " 18,\n",
              " 11,\n",
              " 21,\n",
              " 70,\n",
              " 29,\n",
              " 45,\n",
              " 18,\n",
              " 8,\n",
              " 16,\n",
              " 14,\n",
              " 14,\n",
              " 73,\n",
              " 18,\n",
              " 70,\n",
              " 29,\n",
              " 43,\n",
              " 32,\n",
              " 18,\n",
              " 9,\n",
              " 32,\n",
              " 67,\n",
              " 11,\n",
              " 18,\n",
              " 11,\n",
              " 21,\n",
              " 76,\n",
              " 34,\n",
              " 34,\n",
              " 18,\n",
              " 2,\n",
              " 29,\n",
              " 9,\n",
              " 18,\n",
              " 45,\n",
              " 70,\n",
              " 32,\n",
              " 42,\n",
              " 26,\n",
              " 11,\n",
              " 18,\n",
              " 40,\n",
              " 29,\n",
              " 16,\n",
              " 32,\n",
              " 1,\n",
              " 18,\n",
              " 2,\n",
              " 29,\n",
              " 76,\n",
              " 1,\n",
              " 66,\n",
              " 18,\n",
              " 11,\n",
              " 6,\n",
              " 73,\n",
              " 18,\n",
              " 70,\n",
              " 32,\n",
              " 18,\n",
              " 2,\n",
              " 29,\n",
              " 52,\n",
              " 2,\n",
              " 18,\n",
              " 45,\n",
              " 70,\n",
              " 32,\n",
              " 52,\n",
              " 18,\n",
              " 29,\n",
              " 16,\n",
              " 32,\n",
              " 18,\n",
              " 1,\n",
              " 32,\n",
              " 29,\n",
              " 1,\n",
              " 26,\n",
              " 18,\n",
              " 45,\n",
              " 70,\n",
              " 29,\n",
              " 45,\n",
              " 35,\n",
              " 2,\n",
              " 18,\n",
              " 8,\n",
              " 16,\n",
              " 14,\n",
              " 14,\n",
              " 73,\n",
              " 18,\n",
              " 32,\n",
              " 41,\n",
              " 14,\n",
              " 54,\n",
              " 50,\n",
              " 70,\n",
              " 18,\n",
              " 73,\n",
              " 14,\n",
              " 16,\n",
              " 18,\n",
              " 42,\n",
              " 32,\n",
              " 66,\n",
              " 11,\n",
              " 18,\n",
              " 21,\n",
              " 76,\n",
              " 34,\n",
              " 34,\n",
              " 18,\n",
              " 70,\n",
              " 29,\n",
              " 1,\n",
              " 18,\n",
              " 71,\n",
              " 41,\n",
              " 14,\n",
              " 9,\n",
              " 41,\n",
              " 18,\n",
              " 45,\n",
              " 70,\n",
              " 32,\n",
              " 52,\n",
              " 18,\n",
              " 9,\n",
              " 14,\n",
              " 54,\n",
              " 34,\n",
              " 1,\n",
              " 18,\n",
              " 1,\n",
              " 16,\n",
              " 29,\n",
              " 50,\n",
              " 18,\n",
              " 70,\n",
              " 76,\n",
              " 42,\n",
              " 18,\n",
              " 76,\n",
              " 41,\n",
              " 45,\n",
              " 14,\n",
              " 18,\n",
              " 45,\n",
              " 70,\n",
              " 32,\n",
              " 18,\n",
              " 56,\n",
              " 54,\n",
              " 29,\n",
              " 16,\n",
              " 16,\n",
              " 32,\n",
              " 34,\n",
              " 18,\n",
              " 2,\n",
              " 14,\n",
              " 14,\n",
              " 41,\n",
              " 32,\n",
              " 16,\n",
              " 18,\n",
              " 14,\n",
              " 16,\n",
              " 18,\n",
              " 34,\n",
              " 29,\n",
              " 45,\n",
              " 32,\n",
              " 16,\n",
              " 66,\n",
              " 18,\n",
              " 61,\n",
              " 32,\n",
              " 18,\n",
              " 9,\n",
              " 76,\n",
              " 2,\n",
              " 70,\n",
              " 32,\n",
              " 1,\n",
              " 18,\n",
              " 76,\n",
              " 45,\n",
              " 18,\n",
              " 70,\n",
              " 29,\n",
              " 1,\n",
              " 18,\n",
              " 57,\n",
              " 32,\n",
              " 32,\n",
              " 41,\n",
              " 18,\n",
              " 34,\n",
              " 29,\n",
              " 45,\n",
              " 32,\n",
              " 16,\n",
              " 18,\n",
              " 16,\n",
              " 29,\n",
              " 45,\n",
              " 70,\n",
              " 32,\n",
              " 16,\n",
              " 18,\n",
              " 45,\n",
              " 70,\n",
              " 29,\n",
              " 41,\n",
              " 18,\n",
              " 2,\n",
              " 14,\n",
              " 14,\n",
              " 41,\n",
              " 32,\n",
              " 16,\n",
              " 66,\n",
              " 18,\n",
              " 11,\n",
              " 28,\n",
              " 52,\n",
              " 18,\n",
              " 42,\n",
              " 14,\n",
              " 45,\n",
              " 70,\n",
              " 32,\n",
              " 16,\n",
              " 18,\n",
              " 45,\n",
              " 14,\n",
              " 34,\n",
              " 1,\n",
              " 18,\n",
              " 42,\n",
              " 32,\n",
              " 18,\n",
              " 45,\n",
              " 70,\n",
              " 29,\n",
              " 45,\n",
              " 18,\n",
              " 1,\n",
              " 32,\n",
              " 29,\n",
              " 1,\n",
              " 18,\n",
              " 42,\n",
              " 32,\n",
              " 41,\n",
              " 18,\n",
              " 2,\n",
              " 76,\n",
              " 41,\n",
              " 50,\n",
              " 18,\n",
              " 41,\n",
              " 14,\n",
              " 18,\n",
              " 2,\n",
              " 14,\n",
              " 41,\n",
              " 50,\n",
              " 2,\n",
              " 26,\n",
              " 11,\n",
              " 18,\n",
              " 70,\n",
              " 32,\n",
              " 18,\n",
              " 8,\n",
              " 54,\n",
              " 45,\n",
              " 18,\n",
              " 76,\n",
              " 41,\n",
              " 66,\n",
              " 18,\n",
              " 11,\n",
              " 28,\n",
              " 52,\n",
              " 18,\n",
              " 9,\n",
              " 32,\n",
              " 45,\n",
              " 18,\n",
              " 41,\n",
              " 54,\n",
              " 16,\n",
              " 2,\n",
              " 32,\n",
              " 18,\n",
              " 2,\n",
              " 29,\n",
              " 76,\n",
              " 1,\n",
              " 18,\n",
              " 45,\n",
              " 70,\n",
              " 32,\n",
              " 18,\n",
              " 2,\n",
              " 29,\n",
              " 42,\n",
              " 32,\n",
              " 18,\n",
              " 45,\n",
              " 70,\n",
              " 76,\n",
              " 41,\n",
              " 50,\n",
              " 26,\n",
              " 18,\n",
              " 21,\n",
              " 76,\n",
              " 34,\n",
              " 34,\n",
              " 26,\n",
              " 11,\n",
              " 18,\n",
              " 30,\n",
              " 14,\n",
              " 52,\n",
              " 27,\n",
              " 32,\n",
              " 18,\n",
              " 16,\n",
              " 32,\n",
              " 8,\n",
              " 34,\n",
              " 76,\n",
              " 32,\n",
              " 1,\n",
              " 66,\n",
              " 18,\n",
              " 11,\n",
              " 3,\n",
              " 32,\n",
              " 43,\n",
              " 32,\n",
              " 16,\n",
              " 18,\n",
              " 57,\n",
              " 32,\n",
              " 34,\n",
              " 76,\n",
              " 32,\n",
              " 43,\n",
              " 32,\n",
              " 18,\n",
              " 29,\n",
              " 41,\n",
              " 52,\n",
              " 45,\n",
              " 70,\n",
              " 76,\n",
              " 41,\n",
              " 50,\n",
              " 18,\n",
              " 52,\n",
              " 14,\n",
              " 54,\n",
              " 18,\n",
              " 70,\n",
              " 32,\n",
              " 29,\n",
              " 16,\n",
              " 18,\n",
              " 29,\n",
              " 45,\n",
              " 18,\n",
              " 29,\n",
              " 18,\n",
              " 9,\n",
              " 14,\n",
              " 42,\n",
              " 29,\n",
              " 41,\n",
              " 35,\n",
              " 2,\n",
              " 18,\n",
              " 45,\n",
              " 76,\n",
              " 45,\n",
              " 66,\n",
              " 18,\n",
              " 69,\n",
              " 70,\n",
              " 32,\n",
              " 16,\n",
              " 32,\n",
              " 18,\n",
              " 29,\n",
              " 16,\n",
              " 32,\n",
              " 18,\n",
              " 45,\n",
              " 70,\n",
              " 76,\n",
              " 41,\n",
              " 50,\n",
              " 2,\n",
              " 18,\n",
              " 45,\n",
              " 14,\n",
              " 18,\n",
              " 57,\n",
              " 32,\n",
              " 18,\n",
              " 34,\n",
              " 32,\n",
              " 29,\n",
              " 16,\n",
              " 41,\n",
              " 32,\n",
              " 1,\n",
              " 18,\n",
              " 32,\n",
              " 43,\n",
              " 32,\n",
              " 41,\n",
              " 18,\n",
              " 73,\n",
              " 16,\n",
              " 14,\n",
              " 42,\n",
              " 18,\n",
              " 45,\n",
              " 70,\n",
              " 32,\n",
              " 18,\n",
              " 1,\n",
              " 32,\n",
              " 29,\n",
              " 1,\n",
              " 66,\n",
              " 11,\n",
              " 18,\n",
              " 61,\n",
              " 76,\n",
              " 2,\n",
              " 18,\n",
              " 43,\n",
              " 14,\n",
              " 76,\n",
              " 27,\n",
              " 32,\n",
              " 18,\n",
              " 32,\n",
              " 27,\n",
              " 70,\n",
              " 14,\n",
              " 32,\n",
              " 1,\n",
              " 26,\n",
              " 18,\n",
              " 45,\n",
              " 14,\n",
              " 14,\n",
              " 18,\n",
              " 34,\n",
              " 14,\n",
              " 54,\n",
              " 1,\n",
              " 18,\n",
              " 76,\n",
              " 41,\n",
              " 18,\n",
              " 45,\n",
              " 70,\n",
              " 32,\n",
              " 18,\n",
              " 45,\n",
              " 9,\n",
              " 76,\n",
              " 34,\n",
              " 76,\n",
              " 45,\n",
              " 18,\n",
              " 73,\n",
              " 14,\n",
              " 16,\n",
              " 32,\n",
              " 2,\n",
              " 45,\n",
              " 66,\n",
              " 18,\n",
              " 11,\n",
              " 21,\n",
              " 32,\n",
              " 18,\n",
              " 70,\n",
              " 29,\n",
              " 43,\n",
              " 32,\n",
              " 18,\n",
              " 29,\n",
              " 18,\n",
              " 34,\n",
              " 14,\n",
              " 41,\n",
              " 50,\n",
              " 18,\n",
              " 16,\n",
              " 76,\n",
              " 1,\n",
              " 32,\n",
              " 18,\n",
              " 57,\n",
              " 32,\n",
              " 73,\n",
              " 14,\n",
              " 16,\n",
              " 32,\n",
              " 18,\n",
              " 54,\n",
              " 2,\n",
              " 26,\n",
              " 11,\n",
              " 18,\n",
              " 40,\n",
              " 29,\n",
              " 16,\n",
              " 32,\n",
              " 1,\n",
              " 18,\n",
              " 8,\n",
              " 14,\n",
              " 76,\n",
              " 41,\n",
              " 45,\n",
              " 32,\n",
              " 1,\n",
              " 18,\n",
              " 14,\n",
              " 54,\n",
              " 45,\n",
              " 66,\n",
              " 18,\n",
              " 11,\n",
              " 65,\n",
              " 76,\n",
              " 50,\n",
              " 70,\n",
              " 45,\n",
              " 18,\n",
              " 1,\n",
              " 29,\n",
              " 52,\n",
              " 2,\n",
              " 26,\n",
              " 18,\n",
              " 42,\n",
              " 29,\n",
              " 52,\n",
              " 57,\n",
              " 32,\n",
              " 18,\n",
              " 41,\n",
              " 76,\n",
              " 41,\n",
              " 32,\n",
              " 66,\n",
              " 18,\n",
              " 25,\n",
              " 41,\n",
              " 1,\n",
              " 18,\n",
              " 41,\n",
              " 76,\n",
              " 50,\n",
              " 70,\n",
              " 45,\n",
              " 18,\n",
              " 76,\n",
              " 2,\n",
              " 18,\n",
              " 73,\n",
              " 29,\n",
              " 34,\n",
              " 34,\n",
              " 76,\n",
              " 41,\n",
              " 50,\n",
              " 66,\n",
              " 11,\n",
              " 18,\n",
              " 5,\n",
              " 32,\n",
              " 16,\n",
              " 18,\n",
              " 21,\n",
              " 29,\n",
              " 52,\n",
              " 42,\n",
              " 29,\n",
              " 16,\n",
              " 18,\n",
              " 30,\n",
              " 14]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_text[:1000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfpYcaypKcI9"
      },
      "source": [
        "### Organizando y estructurando el dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "WSSmg9jtKP0T"
      },
      "outputs": [],
      "source": [
        "# separaremos el dataset entre entrenamiento y validación.\n",
        "# `p_val` será la proporción del corpus que se reservará para validación\n",
        "# `num_val` es la cantidad de secuencias de tamaño `max_context_size` que se usará en validación\n",
        "p_val = 0.1\n",
        "num_val = int(np.ceil(len(tokenized_text)*p_val/max_context_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "b7dCpGrdKll0"
      },
      "outputs": [],
      "source": [
        "# separamos la porción de texto utilizada en entrenamiento de la de validación.\n",
        "train_text = tokenized_text[:-num_val*max_context_size]\n",
        "val_text = tokenized_text[-num_val*max_context_size:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "NmxQdxl8LRCg"
      },
      "outputs": [],
      "source": [
        "tokenized_sentences_val = [val_text[init*max_context_size:init*(max_context_size+1)] for init in range(num_val)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "_gyFT9koLqDm"
      },
      "outputs": [],
      "source": [
        "tokenized_sentences_train = [train_text[init:init+max_context_size] for init in range(len(train_text)-max_context_size+1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "oVNqmmLRodT0"
      },
      "outputs": [],
      "source": [
        "X = np.array(tokenized_sentences_train[:-1])\n",
        "y = np.array(tokenized_sentences_train[1:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vken7O4ETsAJ"
      },
      "source": [
        "Nótese que estamos estructurando el problema de aprendizaje como *many-to-many*:\n",
        "\n",
        "Entrada: secuencia de tokens [$x_0$, $x_1$, ..., $x_N$]\n",
        "\n",
        "Target: secuencia de tokens [$x_1$, $x_2$, ..., $x_{N+1}$]\n",
        "\n",
        "De manera que la red tiene que aprender que su salida deben ser los tokens desplazados en una posición y un nuevo token predicho (el N+1).\n",
        "\n",
        "La ventaja de estructurar el aprendizaje de esta manera es que para cada token de target se propaga una señal de gradiente por el grafo de cómputo recurrente, que es mejor que estructurar el problema como *many-to-one* en donde sólo una señal de gradiente se propaga."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3iPTx-UJl6r"
      },
      "source": [
        "En este punto tenemos en la variable `tokenized_sentences` los versos tokenizados. Vamos a quedarnos con un conjunto de validación que utilizaremos para medir la calidad de la generación de secuencias con la métrica de Perplejidad."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "KFAyA4zCWE-5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1424087, 100)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "qcKRl70HFTzG"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([11, 21, 32, 18,  2, 70, 14, 54, 34,  1])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X[0,:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "TVpLCKSZFXZO"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([21, 32, 18,  2, 70, 14, 54, 34,  1, 18])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y[0,:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "wOFCR-KqbW1N"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(chars_vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnnjdAQ5UAEJ"
      },
      "source": [
        "# Definiendo el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "rkMCZvmhrQz4"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Input, TimeDistributed, CategoryEncoding, SimpleRNN, Dense\n",
        "from keras.models import Model, Sequential"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgz7VKwTUbj6"
      },
      "source": [
        "El modelo que se propone como ejemplo consume los índices de los tokens y los transforma en vectores OHE (en este caso no entrenamos una capa de embedding para caracteres). Esa transformación se logra combinando las capas `CategoryEncoding` que transforma a índices a vectores OHE y `TimeDistributed` que aplica la capa a lo largo de la dimensión \"temporal\" de la secuencia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Zd2OkfQYs2Q7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/agustin/Desktop/CEIA UBA/Procesamiento Lenguaje Natural I/CEIA-ProcesamientoLenguajeNaturalI/.venv/lib/python3.12/site-packages/keras/src/layers/core/wrapper.py:27: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1759099183.333947   63909 cuda_executor.cc:1309] INTERNAL: CUDA Runtime error: Failed call to cudaGetRuntimeVersion: Error loading CUDA libraries. GPU will not be used.: Error loading CUDA libraries. GPU will not be used.\n",
            "W0000 00:00:1759099183.342116   63909 gpu_device.cc:2342] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ time_distributed                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">77</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ simple_rnn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">55,600</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">77</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">15,477</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ time_distributed                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m77\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ simple_rnn (\u001b[38;5;33mSimpleRNN\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)      │        \u001b[38;5;34m55,600\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m77\u001b[0m)       │        \u001b[38;5;34m15,477\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">71,077</span> (277.64 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m71,077\u001b[0m (277.64 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">71,077</span> (277.64 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m71,077\u001b[0m (277.64 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(TimeDistributed(CategoryEncoding(num_tokens=vocab_size, output_mode = \"one_hot\"),input_shape=(None,1)))\n",
        "model.add(SimpleRNN(200, return_sequences=True, dropout=0.1, recurrent_dropout=0.1 ))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop')\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmJWNyxQwfCE"
      },
      "source": [
        "\n",
        "### Definir el modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWK3z85sQfUe"
      },
      "source": [
        "Dado que por el momento no hay implementaciones adecuadas de la perplejidad que puedan operar en tiempo de entrenamiento, armaremos un Callback *ad-hoc* que la calcule en cada epoch.\n",
        "\n",
        "**Nota**: un Callback es una rutina gatillada por algún evento, son muy útiles para relevar datos en diferentes momentos del desarrollo del modelo. En este caso queremos hacer un cálculo cada vez que termina una epoch de entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "zUHX3r5JD-MG"
      },
      "outputs": [],
      "source": [
        "class PplCallback(keras.callbacks.Callback):\n",
        "\n",
        "    '''\n",
        "    Este callback es una solución ad-hoc para calcular al final de cada epoch de\n",
        "    entrenamiento la métrica de Perplejidad sobre un conjunto de datos de validación.\n",
        "    La perplejidad es una métrica cuantitativa para evaluar la calidad de la generación de secuencias.\n",
        "    Además implementa la finalización del entrenamiento (Early Stopping)\n",
        "    si la perplejidad no mejora después de `patience` epochs.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, val_data, history_ppl,patience=5):\n",
        "      # El callback lo inicializamos con secuencias de validación sobre las cuales\n",
        "      # mediremos la perplejidad\n",
        "      self.val_data = val_data\n",
        "\n",
        "      self.target = []\n",
        "      self.padded = []\n",
        "\n",
        "      count = 0\n",
        "      self.info = []\n",
        "      self.min_score = np.inf\n",
        "      self.patience_counter = 0\n",
        "      self.patience = patience\n",
        "\n",
        "      # nos movemos en todas las secuencias de los datos de validación\n",
        "      for seq in self.val_data:\n",
        "\n",
        "        len_seq = len(seq)\n",
        "        # armamos todas las subsecuencias\n",
        "        subseq = [seq[:i] for i in range(1,len_seq)]\n",
        "        self.target.extend([seq[i] for i in range(1,len_seq)])\n",
        "\n",
        "        if len(subseq)!=0:\n",
        "\n",
        "          self.padded.append(pad_sequences(subseq, maxlen=max_context_size, padding='pre'))\n",
        "\n",
        "          self.info.append((count,count+len_seq))\n",
        "          count += len_seq\n",
        "\n",
        "      self.padded = np.vstack(self.padded)\n",
        "\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "\n",
        "        # en `scores` iremos guardando la perplejidad de cada secuencia\n",
        "        scores = []\n",
        "\n",
        "        predictions = self.model.predict(self.padded,verbose=0)\n",
        "\n",
        "        # para cada secuencia de validación\n",
        "        for start,end in self.info:\n",
        "\n",
        "          # en `probs` iremos guardando las probabilidades de los términos target\n",
        "          probs = [predictions[idx_seq,-1,idx_vocab] for idx_seq, idx_vocab in zip(range(start,end),self.target[start:end])]\n",
        "\n",
        "          # calculamos la perplejidad por medio de logaritmos\n",
        "          scores.append(np.exp(-np.sum(np.log(probs))/(end-start)))\n",
        "\n",
        "        # promediamos todos los scores e imprimimos el valor promedio\n",
        "        current_score = np.mean(scores)\n",
        "        history_ppl.append(current_score)\n",
        "        print(f'\\n mean perplexity: {current_score} \\n')\n",
        "\n",
        "        # chequeamos si tenemos que detener el entrenamiento\n",
        "        if current_score < self.min_score:\n",
        "          self.min_score = current_score\n",
        "          self.model.save(\"my_model.keras\")\n",
        "          print(\"Saved new model!\")\n",
        "          self.patience_counter = 0\n",
        "        else:\n",
        "          self.patience_counter += 1\n",
        "          if self.patience_counter == self.patience:\n",
        "            print(\"Stopping training...\")\n",
        "            self.model.stop_training = True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HBZIwR0gruA"
      },
      "source": [
        "### Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQq1PHDkxDvN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m113/696\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:25\u001b[0m 353ms/step - loss: 3.1593"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# fiteamos, nótese el agregado del callback con su inicialización. El batch_size lo podemos seleccionar a mano\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# en general, lo mejor es escoger el batch más grande posible que minimice el tiempo de cada época.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# En la variable `history_ppl` se guardarán los valores de perplejidad para cada época.\u001b[39;00m\n\u001b[32m      4\u001b[39m history_ppl = []\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m hist = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mPplCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_sentences_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43mhistory_ppl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2048\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CEIA UBA/Procesamiento Lenguaje Natural I/CEIA-ProcesamientoLenguajeNaturalI/.venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CEIA UBA/Procesamiento Lenguaje Natural I/CEIA-ProcesamientoLenguajeNaturalI/.venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:377\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    376\u001b[39m     callbacks.on_train_batch_begin(begin_step)\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m     callbacks.on_train_batch_end(end_step, logs)\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CEIA UBA/Procesamiento Lenguaje Natural I/CEIA-ProcesamientoLenguajeNaturalI/.venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:220\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    217\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    218\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    219\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    222\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CEIA UBA/Procesamiento Lenguaje Natural I/CEIA-ProcesamientoLenguajeNaturalI/.venv/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CEIA UBA/Procesamiento Lenguaje Natural I/CEIA-ProcesamientoLenguajeNaturalI/.venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CEIA UBA/Procesamiento Lenguaje Natural I/CEIA-ProcesamientoLenguajeNaturalI/.venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CEIA UBA/Procesamiento Lenguaje Natural I/CEIA-ProcesamientoLenguajeNaturalI/.venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CEIA UBA/Procesamiento Lenguaje Natural I/CEIA-ProcesamientoLenguajeNaturalI/.venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CEIA UBA/Procesamiento Lenguaje Natural I/CEIA-ProcesamientoLenguajeNaturalI/.venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CEIA UBA/Procesamiento Lenguaje Natural I/CEIA-ProcesamientoLenguajeNaturalI/.venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CEIA UBA/Procesamiento Lenguaje Natural I/CEIA-ProcesamientoLenguajeNaturalI/.venv/lib/python3.12/site-packages/tensorflow/python/eager/context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CEIA UBA/Procesamiento Lenguaje Natural I/CEIA-ProcesamientoLenguajeNaturalI/.venv/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# fiteamos, nótese el agregado del callback con su inicialización. El batch_size lo podemos seleccionar a mano\n",
        "# en general, lo mejor es escoger el batch más grande posible que minimice el tiempo de cada época.\n",
        "# En la variable `history_ppl` se guardarán los valores de perplejidad para cada época.\n",
        "history_ppl = []\n",
        "hist = model.fit(X, y, epochs=20, callbacks=[PplCallback(tokenized_sentences_val,history_ppl)], batch_size=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K30JHB3Dv-mx"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Entrenamiento\n",
        "epoch_count = range(1, len(history_ppl) + 1)\n",
        "sns.lineplot(x=epoch_count,  y=history_ppl)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rhy5hZN38qfO"
      },
      "outputs": [],
      "source": [
        "# Cargamos el mejor modelo guardado del entrenamiento para hacer inferencia\n",
        "model = keras.models.load_model('my_model.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN6Fg_BsxJe6"
      },
      "source": [
        "\n",
        "### Predicción del próximo caracter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNyBykvhzs7-"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def model_response(human_text):\n",
        "\n",
        "    # Encodeamos\n",
        "    encoded = [char2idx[ch] for ch in human_text.lower() ]\n",
        "    # Si tienen distinto largo\n",
        "    encoded = pad_sequences([encoded], maxlen=max_context_size, padding='pre')\n",
        "\n",
        "    # Predicción softmax\n",
        "    y_hat = np.argmax(model.predict(encoded)[0,-1,:])\n",
        "\n",
        "\n",
        "    # Debemos buscar en el vocabulario el caracter\n",
        "    # que corresopnde al indice (y_hat) predicho por le modelo\n",
        "    out_word = ''\n",
        "    out_word = idx2char[y_hat]\n",
        "\n",
        "    # Agrego la palabra a la frase predicha\n",
        "    return human_text + out_word\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=model_response,\n",
        "    inputs=[\"textbox\"],\n",
        "    outputs=\"text\")\n",
        "\n",
        "iface.launch(debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCeMWWupxN1-"
      },
      "source": [
        "### Generación de secuencias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwbS_pfhxvB3"
      },
      "outputs": [],
      "source": [
        "def generate_seq(model, seed_text, max_length, n_words):\n",
        "    \"\"\"\n",
        "        Exec model sequence prediction\n",
        "\n",
        "        Args:\n",
        "            model (keras): modelo entrenado\n",
        "            seed_text (string): texto de entrada (input_seq)\n",
        "            max_length (int): máxima longitud de la sequencia de entrada\n",
        "            n_words (int): números de caracteres a agregar a la sequencia de entrada\n",
        "        returns:\n",
        "            output_text (string): sentencia con las \"n_words\" agregadas\n",
        "    \"\"\"\n",
        "    output_text = seed_text\n",
        "\t# generate a fixed number of words\n",
        "    for _ in range(n_words):\n",
        "\t\t# Encodeamos\n",
        "        encoded = [char2idx[ch] for ch in output_text.lower() ]\n",
        "\t\t# Si tienen distinto largo\n",
        "        encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
        "\n",
        "\t\t# Predicción softmax\n",
        "        y_hat = np.argmax(model.predict(encoded,verbose=0)[0,-1,:])\n",
        "\t\t# Vamos concatenando las predicciones\n",
        "        out_word = ''\n",
        "\n",
        "        out_word = idx2char[y_hat]\n",
        "\n",
        "\t\t# Agrego las palabras a la frase predicha\n",
        "        output_text += out_word\n",
        "    return output_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoFqRC5pxzqS"
      },
      "outputs": [],
      "source": [
        "input_text='habia una vez'\n",
        "\n",
        "generate_seq(model, input_text, max_length=max_context_size, n_words=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drJ6xn5qW1Hl"
      },
      "source": [
        "###  Beam search y muestreo aleatorio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vovn9XZW1Hl"
      },
      "outputs": [],
      "source": [
        "# funcionalidades para hacer encoding y decoding\n",
        "\n",
        "def encode(text,max_length=max_context_size):\n",
        "\n",
        "    encoded = [char2idx[ch] for ch in text]\n",
        "    encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
        "\n",
        "    return encoded\n",
        "\n",
        "def decode(seq):\n",
        "    return ''.join([idx2char[ch] for ch in seq])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_lZiQwkW1Hl"
      },
      "outputs": [],
      "source": [
        "from scipy.special import softmax\n",
        "\n",
        "# función que selecciona candidatos para el beam search\n",
        "def select_candidates(pred,num_beams,vocab_size,history_probs,history_tokens,temp,mode):\n",
        "\n",
        "  # colectar todas las probabilidades para la siguiente búsqueda\n",
        "  pred_large = []\n",
        "\n",
        "  for idx,pp in enumerate(pred):\n",
        "    pred_large.extend(np.log(pp+1E-10)+history_probs[idx])\n",
        "\n",
        "  pred_large = np.array(pred_large)\n",
        "\n",
        "  # criterio de selección\n",
        "  if mode == 'det':\n",
        "    idx_select = np.argsort(pred_large)[::-1][:num_beams] # beam search determinista\n",
        "  elif mode == 'sto':\n",
        "    idx_select = np.random.choice(np.arange(pred_large.shape[0]), num_beams, p=softmax(pred_large/temp)) # beam search con muestreo aleatorio\n",
        "  else:\n",
        "    raise ValueError(f'Wrong selection mode. {mode} was given. det and sto are supported.')\n",
        "\n",
        "  # traducir a índices de token en el vocabulario\n",
        "  new_history_tokens = np.concatenate((np.array(history_tokens)[idx_select//vocab_size],\n",
        "                        np.array([idx_select%vocab_size]).T),\n",
        "                      axis=1)\n",
        "\n",
        "  # devolver el producto de las probabilidades (log) y la secuencia de tokens seleccionados\n",
        "  return pred_large[idx_select.astype(int)], new_history_tokens.astype(int)\n",
        "\n",
        "\n",
        "def beam_search(model,num_beams,num_words,input,temp=1,mode='det'):\n",
        "\n",
        "    # first iteration\n",
        "\n",
        "    # encode\n",
        "    encoded = encode(input)\n",
        "\n",
        "    # first prediction\n",
        "    y_hat = model.predict(encoded,verbose=0)[0,-1,:]\n",
        "\n",
        "    # get vocabulary size\n",
        "    vocab_size = y_hat.shape[0]\n",
        "\n",
        "    # initialize history\n",
        "    history_probs = [0]*num_beams\n",
        "    history_tokens = [encoded[0]]*num_beams\n",
        "\n",
        "    # select num_beams candidates\n",
        "    history_probs, history_tokens = select_candidates([y_hat],\n",
        "                                        num_beams,\n",
        "                                        vocab_size,\n",
        "                                        history_probs,\n",
        "                                        history_tokens,\n",
        "                                        temp,\n",
        "                                        mode)\n",
        "\n",
        "    # beam search loop\n",
        "    for i in range(num_words-1):\n",
        "\n",
        "      preds = []\n",
        "\n",
        "      for hist in history_tokens:\n",
        "\n",
        "        # actualizar secuencia de tokens\n",
        "        input_update = np.array([hist[i+1:]]).copy()\n",
        "\n",
        "        # predicción\n",
        "        y_hat = model.predict(input_update,verbose=0)[0,-1,:]\n",
        "\n",
        "        preds.append(y_hat)\n",
        "\n",
        "      history_probs, history_tokens = select_candidates(preds,\n",
        "                                                        num_beams,\n",
        "                                                        vocab_size,\n",
        "                                                        history_probs,\n",
        "                                                        history_tokens,\n",
        "                                                        temp,\n",
        "                                                        mode)\n",
        "\n",
        "    return history_tokens[:,-(len(input)+num_words):]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GeLqAoOYW1Hm"
      },
      "outputs": [],
      "source": [
        "# predicción con beam search\n",
        "salidas = beam_search(model,num_beams=10,num_words=20,input=\"habia una vez\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8HQoLhw-NYg"
      },
      "outputs": [],
      "source": [
        "salidas[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2S3_I3S1W1Hm"
      },
      "outputs": [],
      "source": [
        "# veamos las salidas\n",
        "decode(salidas[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_LlqmtEW1Hn"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
